{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up blank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "import json\n",
    "# create target model configuration\n",
    "target_model_config = {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForSequenceClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": None,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"finetuning_task\": \"glue:rte\",\n",
    "  \"gradient_checkpointing\": False,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 15,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "#create blank model\n",
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "from transformers import RobertaConfig\n",
    "#define donor model\n",
    "donor_model_for_classification = TFRobertaForSequenceClassification.from_pretrained(\"textattack/roberta-base-MNLI\", from_pt=True)\n",
    "new_config = RobertaConfig(**target_model_config)\n",
    "blank_model = TFRobertaForSequenceClassification(new_config)\n",
    "blank_model.build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLaye  multiple                  145318656 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " classifier (TFRobertaClass  multiple                  592130    \n",
      " ificationHead)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145910786 (556.61 MB)\n",
      "Trainable params: 145910786 (556.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#NEW\n",
    "import json\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "import random\n",
    "# Create target model configuration\n",
    "target_model_config = {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForSequenceClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": None,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"finetuning_task\": \"glue:rte\",\n",
    "  \"gradient_checkpointing\": False,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 15,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "\n",
    "# Load the donor model with PyTorch weights\n",
    "donor_model_name = random.choice([\"textattack/roberta-base-MNLI\", \"textattack/roberta-base-RTE\"])\n",
    "donor_model_for_classification_head = TFRobertaForSequenceClassification.from_pretrained(donor_model_name, from_pt=True)\n",
    "\n",
    "# Create the blank model\n",
    "new_config = RobertaConfig(**target_model_config)\n",
    "blank_model = TFRobertaForSequenceClassification(new_config)\n",
    "blank_model.build()\n",
    "\n",
    "# Explore the model structure to find the correct layer\n",
    "print(blank_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_model.save_pretrained('blank_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define weave pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_model_template': './blank_model',\n",
       " 'layer_assignments': [{'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 0}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 1}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 1}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 3}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 4}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 8}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 7}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 10}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 11}}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from random import randint\n",
    "# Generate model weaving configuration\n",
    "model_weaving_config = {\n",
    "    # The task (i.e. the classification head should match the task at hand)\n",
    "    \"target_model_template\": \"./blank_model\",\n",
    "    # Layer assignments\n",
    "    \"layer_assignments\": [\n",
    "        {\n",
    "            \"type\": \"SingleLayer\",\n",
    "            \"params\": {\n",
    "                # Load donor model && Pick a model randomly, p=0.5\n",
    "                \"donor\": [\"textattack/roberta-base-MNLI\",\n",
    "                          \"textattack/roberta-base-RTE\"][randint(0, 1)],\n",
    "                # Pick a layer within [i-1,i+1], keeping it between 0 and 11\n",
    "                \"hidden_layer_number\": min(11, max(0, randint(i - 1,i + 1))),\n",
    "            },\n",
    "        } for i in range(15)\n",
    "    ],\n",
    "}\n",
    "\n",
    "model_weaving_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from llm_weaver.weave import weave_models\n",
    "\n",
    "weaved_model = weave_models(target_model_template=blank_model, layer_assignments=model_weaving_config[\"layer_assignments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaved_model.save_pretrained('weaved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (4.9.3)\n",
      "Requirement already satisfied: absl-py in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: array-record in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (0.4.1)\n",
      "Requirement already satisfied: click in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.5.2)\n",
      "Requirement already satisfied: numpy in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (1.26.0)\n",
      "Requirement already satisfied: promise in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (4.66.1)\n",
      "Requirement already satisfied: wrapt in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: fsspec in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.10.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.1.0)\n",
      "Requirement already satisfied: typing_extensions in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.7.1)\n",
      "Requirement already satisfied: zipp in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2023.7.22)\n",
      "Requirement already satisfied: six in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /Users/hivamoh/anaconda3/envs/tensorzipper/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.61.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at weaved_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "ename": "UnparsedFlagAccessError",
     "evalue": "Trying to access flag --glue_task before flags were parsed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 110\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAGS\u001b[38;5;241m.\u001b[39mfavor_target_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[FLAGS\u001b[38;5;241m.\u001b[39mfavor_target_model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 110\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 88\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m     87\u001b[0m ds \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_glue_dataset(\n\u001b[0;32m---> 88\u001b[0m     task\u001b[38;5;241m=\u001b[39m\u001b[43mFLAGS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglue_task\u001b[49m,\n\u001b[1;32m     89\u001b[0m     split\u001b[38;5;241m=\u001b[39mFLAGS\u001b[38;5;241m.\u001b[39msplit,\n\u001b[1;32m     90\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mweaved_model\u001b[38;5;241m.\u001b[39mtokenizer,\n\u001b[1;32m     91\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mFLAGS\u001b[38;5;241m.\u001b[39msequence_length,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     93\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mtake(FLAGS\u001b[38;5;241m.\u001b[39mn_examples)\u001b[38;5;241m.\u001b[39mbatch(FLAGS\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Load metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorzipper/lib/python3.9/site-packages/absl/flags/_flagvalues.py:481\u001b[0m, in \u001b[0;36mFlagValues.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[39mreturn\u001b[39;00m fl[name]\u001b[39m.\u001b[39mvalue\n\u001b[1;32m    480\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 481\u001b[0m   \u001b[39mraise\u001b[39;00m _exceptions\u001b[39m.\u001b[39mUnparsedFlagAccessError(\n\u001b[1;32m    482\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mTrying to access flag --\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m before flags were parsed.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m name)\n",
      "\u001b[0;31mUnparsedFlagAccessError\u001b[0m: Trying to access flag --glue_task before flags were parsed."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "import tensorflow as tf\n",
    "from absl import app, flags\n",
    "from transformers.data.processors import glue as hf_glue\n",
    "# Example import statement assuming 'evaluation' is a module within 'model_merging'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "#flags.DEFINE_string(\"glue_task_weave_model\", \"rte\", \"GLUE task for evaluation\")\n",
    "#flags.DEFINE_string(\"split_weave_model\", \"validation\", \"Data split for evaluation\")\n",
    "#flags.DEFINE_integer(\"n_examples_weave_model\", 1000, \"Number of examples to evaluate\")\n",
    "#flags.DEFINE_integer(\"batch_size_weave_model\", 32, \"Batch size for evaluation\")\n",
    "#flags.DEFINE_integer(\"sequence_length_weave_model\", 128, \"Maximum sequence length\")\n",
    "#flags.DEFINE_string(\"favor_target_model_weave_model\", \"accuracy\", \"Favor target model based on the metric (e.g., 'accuracy')\")\n",
    "#flags.DEFINE_boolean(\"normalize_fishers_weave_model\", True, \"Normalize Fisher scores\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from model_merging import data\n",
    "from model_merging.model_merging import evaluation\n",
    "from model_merging.model_merging import data\n",
    "flags.DEFINE_string(\"glue_task\", None, \"\")\n",
    "#from model_merging.model_merging.evaluation import load_metric_for_glue_task, evaluate_model\n",
    "#from model_merging.evaluation import load_metric_for_glue_task, evaluate_model\n",
    "\n",
    "#from model_merging import hdf5_util\n",
    "#from model_merging import merging\n",
    "\n",
    "def _to_tfds_task_name(task, split):\n",
    "    if task == \"sts-b\":\n",
    "        task = \"stsb\"\n",
    "    elif task == \"sst-2\":\n",
    "        task = \"sst2\"\n",
    "    elif task == \"mnli\" and split != \"train\":\n",
    "        task = \"mnli_matched\"\n",
    "    elif task == \"mnli-mm\" and split != \"train\":\n",
    "        task = \"mnli_mismatched\"\n",
    "    return task\n",
    "\n",
    "_STSB_MIN = 0\n",
    "_STSB_MAX = 5\n",
    "_STSB_NUM_BINS = 5 * (_STSB_MAX - _STSB_MIN)\n",
    "\n",
    "def _convert_dataset_to_features(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    task,\n",
    "):\n",
    "    \"\"\"Note that this is only for single examples; won't work with batched inputs.\"\"\"\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "    # NOTE: Not sure if this is correct, but it matches up for BERT. RoBERTa does\n",
    "    # not appear to use token types\n",
    "    pad_token_segment_id = tokenizer.pad_token_type_id\n",
    "    _glue_processors = hf_glue.glue_processors\n",
    "    _glue_output_modes = hf_glue.glue_output_modes\n",
    "    processor = _glue_processors[task]()\n",
    "    output_mode = _glue_output_modes[task]\n",
    "\n",
    "    if task == \"sts-b\":\n",
    "        # STS-B regression\n",
    "        stsb_bins = np.linspace(_STSB_MIN, _STSB_MAX, num=_STSB_NUM_BINS + 1)\n",
    "        stsb_bins = stsb_bins[1:-1]\n",
    "    else:\n",
    "        label_list = processor.get_labels()\n",
    "        label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "\n",
    "def load_glue_dataset(task: str, split: str, tokenizer, max_length: int):\n",
    "    tfds_task = _to_tfds_task_name(task, split)\n",
    "    ds = tf.load(f\"glue/{tfds_task}\", split=split)\n",
    "    ds = _convert_dataset_to_features(\n",
    "        ds,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        task,\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def main():\n",
    "    # Load the weaved model\n",
    "    weaved_model = TFRobertaForSequenceClassification.from_pretrained('weaved_model')\n",
    "    print('hi')\n",
    "    # Load the dataset\n",
    "    ds = data.load_glue_dataset(\n",
    "        task=FLAGS.glue_task,\n",
    "        split=FLAGS.split,\n",
    "        tokenizer=weaved_model.tokenizer,\n",
    "        max_length=FLAGS.sequence_length,\n",
    "    )\n",
    "    ds = ds.take(FLAGS.n_examples).batch(FLAGS.batch_size)\n",
    "\n",
    "    # Load metrics\n",
    "    metric = evaluation.load_metric_for_glue_task(FLAGS.glue_task)\n",
    "\n",
    "    # Evaluate the weaved model\n",
    "    results = evaluation.evaluate_model(weaved_model, ds, metric)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(80 * \"*\")\n",
    "    print(\" Weaved Model Evaluation\")\n",
    "    print(80 * \"*\")\n",
    "    print(f\"{FLAGS.glue_task} {FLAGS.split} {FLAGS.n_examples} Examples\")\n",
    "    print(80 * \"-\")\n",
    "    print(f\"Metric: {FLAGS.favor_target_model}\")\n",
    "    print(f\"{metric.name}: {results[FLAGS.favor_target_model]}\")\n",
    "\n",
    "main()\n",
    "#if __name__ == \"__main__\":\n",
    "#    app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('tensorzipper')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ccdc7606d6b7c6e8f11aa84e2b7f90d58ab1574f473c12a514bac348cc8383a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
