{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up blank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "target_model_config = {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForSequenceClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": None,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"finetuning_task\": \"glue:rte\",\n",
    "  \"gradient_checkpointing\": False,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 15,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "\n",
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "new_config = RobertaConfig(**target_model_config)\n",
    "blank_model = TFRobertaForSequenceClassification(new_config)\n",
    "blank_model.build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_model.save_pretrained('blank_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define weave pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_model_template': './blank_model',\n",
       " 'layer_assignments': [{'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 0}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 0}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 2}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 2}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 6}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 8}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 9}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 8}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 9}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 11}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from random import randint\n",
    "model_weaving_config = {\n",
    "    # The task (i.e. the classification head should match the task at hand)\n",
    "    \"target_model_template\": \"./blank_model\",\n",
    "    # layer assignments\n",
    "    \"layer_assignments\": [\n",
    "        {\n",
    "            \"type\": \"SingleLayer\",\n",
    "            \"params\": {\n",
    "                # Pick one model randomly, p=0.5\n",
    "                \"donor\": [\"textattack/roberta-base-MNLI\",\n",
    "                          \"textattack/roberta-base-RTE\"][randint(0, 1)],\n",
    "                # Pick a layer within [i-1,i+1], keeping it between 0 and 11\n",
    "                \"hidden_layer_number\": min(11, max(0, randint(i - 1,i + 1))),\n",
    "            },\n",
    "        } for i in range(15)\n",
    "    ],\n",
    "}\n",
    "\n",
    "model_weaving_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from llm_weaver.weave import weave_models\n",
    "\n",
    "weaved_model = weave_models(target_model_template=blank_model, layer_assignments=model_weaving_config[\"layer_assignments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaved_model.save_pretrained('weaved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "??"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
