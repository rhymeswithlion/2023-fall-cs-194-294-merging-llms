{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up blank model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# OLD\n",
    "import json\n",
    "# create target model configuration\n",
    "target_model_config = {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForSequenceClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": None,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"finetuning_task\": \"glue:rte\",\n",
    "  \"gradient_checkpointing\": False,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 15,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "#create blank model\n",
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "from transformers import RobertaConfig\n",
    "#define donor model\n",
    "donor_model_for_classification = TFRobertaForSequenceClassification.from_pretrained(\"textattack/roberta-base-MNLI\", from_pt=True)\n",
    "new_config = RobertaConfig(**target_model_config)\n",
    "blank_model = TFRobertaForSequenceClassification(new_config)\n",
    "blank_model.build()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLaye  multiple                  145318656 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " classifier (TFRobertaClass  multiple                  592130    \n",
      " ificationHead)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145910786 (556.61 MB)\n",
      "Trainable params: 145910786 (556.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#NEW\n",
    "import json\n",
    "from transformers import RobertaForSequenceClassification, RobertaConfig\n",
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "import random\n",
    "# Create target model configuration\n",
    "target_model_config = {\n",
    "  \"architectures\": [\n",
    "    \"RobertaForSequenceClassification\"\n",
    "  ],\n",
    "  \"attention_probs_dropout_prob\": 0.1,\n",
    "  \"bos_token_id\": 0,\n",
    "  \"classifier_dropout\": None,\n",
    "  \"eos_token_id\": 2,\n",
    "  \"finetuning_task\": \"glue:rte\",\n",
    "  \"gradient_checkpointing\": False,\n",
    "  \"hidden_act\": \"gelu\",\n",
    "  \"hidden_dropout_prob\": 0.1,\n",
    "  \"hidden_size\": 768,\n",
    "  \"initializer_range\": 0.02,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"layer_norm_eps\": 1e-05,\n",
    "  \"max_position_embeddings\": 514,\n",
    "  \"model_type\": \"roberta\",\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 15,\n",
    "  \"pad_token_id\": 1,\n",
    "  \"position_embedding_type\": \"absolute\",\n",
    "  \"transformers_version\": \"4.35.0\",\n",
    "  \"type_vocab_size\": 1,\n",
    "  \"use_cache\": True,\n",
    "  \"vocab_size\": 50265\n",
    "}\n",
    "\n",
    "# Load the donor model with PyTorch weights\n",
    "donor_model_name = random.choice([\"textattack/roberta-base-MNLI\", \"textattack/roberta-base-RTE\"])\n",
    "donor_model_for_classification_head = TFRobertaForSequenceClassification.from_pretrained(donor_model_name, from_pt=True)\n",
    "\n",
    "# Create the blank model\n",
    "new_config = RobertaConfig(**target_model_config)\n",
    "blank_model = TFRobertaForSequenceClassification(new_config)\n",
    "blank_model.build()\n",
    "\n",
    "# Explore the model structure to find the correct layer\n",
    "print(blank_model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_model.save_pretrained('blank_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define weave pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_model_template': './blank_model',\n",
       " 'layer_assignments': [{'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 0}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 1}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 2}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 3}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 8}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 8}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 10}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 10}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 11}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from random import randint\n",
    "# Generate model weaving configuration\n",
    "model_weaving_config = {\n",
    "    # The task (i.e. the classification head should match the task at hand)\n",
    "    \"target_model_template\": \"./blank_model\",\n",
    "    # Layer assignments\n",
    "    \"layer_assignments\": [\n",
    "        {\n",
    "            \"type\": \"SingleLayer\",\n",
    "            \"params\": {\n",
    "                # Load donor model && Pick a model randomly, p=0.5\n",
    "                \"donor\": [\"textattack/roberta-base-MNLI\",\n",
    "                          \"textattack/roberta-base-RTE\"][randint(0, 1)],\n",
    "                # Pick a layer within [i-1,i+1], keeping it between 0 and 11\n",
    "                \"hidden_layer_number\": min(11, max(0, randint(i - 1,i + 1))),\n",
    "            },\n",
    "        } for i in range(15)\n",
    "    ],\n",
    "}\n",
    "\n",
    "model_weaving_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from llm_weaver.weave import weave_models\n",
    "\n",
    "weaved_model = weave_models(target_model_template=blank_model, layer_assignments=model_weaving_config[\"layer_assignments\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaved_model.save_pretrained('weaved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model_merging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#from model_merging import data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel_merging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_merging\u001b[39;00m \u001b[39mimport\u001b[39;00m evaluation\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#from model_merging.model_merging.evaluation import load_metric_for_glue_task, evaluate_model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#from model_merging.evaluation import load_metric_for_glue_task, evaluate_model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#from model_merging import hdf5_util\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#from model_merging import merging\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marencordts/Documents/GitHub/2023-fall-cs-194-294-merging-llms/notebooks/end-to-end-idea.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_tfds_task_name\u001b[39m(task, split):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model_merging'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from absl import app, flags\n",
    "from transformers.data.processors import glue as hf_glue\n",
    "# Example import statement assuming 'evaluation' is a module within 'model_merging'\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "#flags.DEFINE_string(\"glue_task_weave_model\", \"rte\", \"GLUE task for evaluation\")\n",
    "#flags.DEFINE_string(\"split_weave_model\", \"validation\", \"Data split for evaluation\")\n",
    "#flags.DEFINE_integer(\"n_examples_weave_model\", 1000, \"Number of examples to evaluate\")\n",
    "#flags.DEFINE_integer(\"batch_size_weave_model\", 32, \"Batch size for evaluation\")\n",
    "#flags.DEFINE_integer(\"sequence_length_weave_model\", 128, \"Maximum sequence length\")\n",
    "#flags.DEFINE_string(\"favor_target_model_weave_model\", \"accuracy\", \"Favor target model based on the metric (e.g., 'accuracy')\")\n",
    "#flags.DEFINE_boolean(\"normalize_fishers_weave_model\", True, \"Normalize Fisher scores\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#from model_merging import data\n",
    "from model_merging.model_merging import evaluation\n",
    "#from model_merging.model_merging.evaluation import load_metric_for_glue_task, evaluate_model\n",
    "#from model_merging.evaluation import load_metric_for_glue_task, evaluate_model\n",
    "\n",
    "#from model_merging import hdf5_util\n",
    "#from model_merging import merging\n",
    "\n",
    "def _to_tfds_task_name(task, split):\n",
    "    if task == \"sts-b\":\n",
    "        task = \"stsb\"\n",
    "    elif task == \"sst-2\":\n",
    "        task = \"sst2\"\n",
    "    elif task == \"mnli\" and split != \"train\":\n",
    "        task = \"mnli_matched\"\n",
    "    elif task == \"mnli-mm\" and split != \"train\":\n",
    "        task = \"mnli_mismatched\"\n",
    "    return task\n",
    "\n",
    "_STSB_MIN = 0\n",
    "_STSB_MAX = 5\n",
    "_STSB_NUM_BINS = 5 * (_STSB_MAX - _STSB_MIN)\n",
    "\n",
    "def _convert_dataset_to_features(\n",
    "    dataset,\n",
    "    tokenizer,\n",
    "    max_length,\n",
    "    task,\n",
    "):\n",
    "    \"\"\"Note that this is only for single examples; won't work with batched inputs.\"\"\"\n",
    "    pad_token = tokenizer.pad_token_id\n",
    "    # NOTE: Not sure if this is correct, but it matches up for BERT. RoBERTa does\n",
    "    # not appear to use token types\n",
    "    pad_token_segment_id = tokenizer.pad_token_type_id\n",
    "    _glue_processors = hf_glue.glue_processors\n",
    "    _glue_output_modes = hf_glue.glue_output_modes\n",
    "    processor = _glue_processors[task]()\n",
    "    output_mode = _glue_output_modes[task]\n",
    "\n",
    "    if task == \"sts-b\":\n",
    "        # STS-B regression\n",
    "        stsb_bins = np.linspace(_STSB_MIN, _STSB_MAX, num=_STSB_NUM_BINS + 1)\n",
    "        stsb_bins = stsb_bins[1:-1]\n",
    "    else:\n",
    "        label_list = processor.get_labels()\n",
    "        label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "\n",
    "def load_glue_dataset(task: str, split: str, tokenizer, max_length: int):\n",
    "    tfds_task = _to_tfds_task_name(task, split)\n",
    "    ds = tf.load(f\"glue/{tfds_task}\", split=split)\n",
    "    ds = _convert_dataset_to_features(\n",
    "        ds,\n",
    "        tokenizer,\n",
    "        max_length,\n",
    "        task,\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def main(_):\n",
    "    # Load the weaved model\n",
    "    weaved_model = TFRobertaForSequenceClassification.from_pretrained('weaved_model')\n",
    "\n",
    "    # Load the dataset\n",
    "    ds = load_glue_dataset(\n",
    "        task=FLAGS.glue_task,\n",
    "        split=FLAGS.split,\n",
    "        tokenizer=weaved_model.tokenizer,\n",
    "        max_length=FLAGS.sequence_length,\n",
    "    )\n",
    "    ds = ds.take(FLAGS.n_examples).batch(FLAGS.batch_size)\n",
    "\n",
    "    # Load metrics\n",
    "    metric = evaluation.load_metric_for_glue_task(FLAGS.glue_task)\n",
    "\n",
    "    # Evaluate the weaved model\n",
    "    results = evaluation.evaluate_model(weaved_model, ds, metric)\n",
    "\n",
    "    # Print evaluation results\n",
    "    print(80 * \"*\")\n",
    "    print(\" Weaved Model Evaluation\")\n",
    "    print(80 * \"*\")\n",
    "    print(f\"{FLAGS.glue_task} {FLAGS.split} {FLAGS.n_examples} Examples\")\n",
    "    print(80 * \"-\")\n",
    "    print(f\"Metric: {FLAGS.favor_target_model}\")\n",
    "    print(f\"{metric.name}: {results[FLAGS.favor_target_model]}\")\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
