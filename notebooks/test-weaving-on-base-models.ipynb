{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the weaving code on the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: visions 0.7.5 does not provide the extra 'type-image-path'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "\n",
    "! pip install -q joblib  # joblib for memoizing functions\n",
    "! pip install -q ipywidgets widgetsnbextension pandas-profiling # IProgress for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model_merging to the python path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "model_merging_base = os.path.abspath(\"../model_merging/\")\n",
    "# assert it exist\n",
    "assert os.path.exists(model_merging_base)\n",
    "if model_merging_base not in sys.path:\n",
    "    sys.path.append(model_merging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib for caching and distributed computing\n",
    "from math import sqrt\n",
    "\n",
    "from joblib import Memory, Parallel, delayed\n",
    "\n",
    "# memory = Memory(location=\"cache\", verbose=10)\n",
    "memory = Memory(location=\"cache\", verbose=0)\n",
    "\n",
    "parallel = Parallel(n_jobs=2, return_as=\"generator\")\n",
    "output_generator = parallel(delayed(sqrt)(i**2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cached functions\n",
    "\n",
    "import os\n",
    "\n",
    "from llm_weaver import (\n",
    "    calculate_score_from_weaving_config,\n",
    "    get_score_from_named_model,\n",
    "    test_weaver,\n",
    ")\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "calculate_score_from_weaving_config_cached = memory.cache(\n",
    "    calculate_score_from_weaving_config\n",
    ")\n",
    "test_weaver_cached = memory.cache(test_weaver)\n",
    "\n",
    "get_score_from_named_model_cached = memory.cache(get_score_from_named_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure you can build using `.build()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have transformers version 4.35.0!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from llm_weaver import get_blank_model, get_model_config\n",
    "\n",
    "if transformers.__version__ < \"4.3.1\":\n",
    "    raise ValueError(\n",
    "        \"Need transformers >= 4.3.1, or something like that. Not sure of the version.\"\n",
    "    )\n",
    "    # https://github.com/huggingface/transformers/commit/4a55e4787760fdb6c40a972a60d814ba05425da1#diff-648ec06beb5ae6380c7f611a0f513a5d392509497d245a09f06b6549358afdffR1151\n",
    "\n",
    "print(f\"You have transformers version {transformers.__version__}!\")\n",
    "\n",
    "model = get_blank_model(get_model_config(\"textattack/roberta-base-RTE\"))\n",
    "model.build()\n",
    "\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Test weaving code\n",
    "\n",
    "This test makes sure that our score when using the weaver to reconstruct a model from all its parts get the same evaluation score as the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>({'accuracy': 0.7}, {'accuracy': 0.7})</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>({'accuracy': 0.3}, {'accuracy': 0.3})</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id                                 results\n",
       "0   textattack/roberta-base-RTE  ({'accuracy': 0.7}, {'accuracy': 0.7})\n",
       "1  textattack/roberta-base-MNLI  ({'accuracy': 0.3}, {'accuracy': 0.3})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"textattack/roberta-base-RTE\",\n",
    "    \"textattack/roberta-base-MNLI\",  # <--- this one has a very low score\n",
    "    # \"howey/roberta-large-rte\",\n",
    "    # \"howey/roberta-large-mnli\",\n",
    "    # \"howey/roberta-large-qnli\",\n",
    "    # \"howey/roberta-large-sst2\",\n",
    "    # \"howey/roberta-large-cola\",\n",
    "    # \"howey/roberta-large-mrpc\",\n",
    "    # \"howey/roberta-large-qqp\",\n",
    "    # \"howey/roberta-large-stsb\",\n",
    "    # \"JeremiahZ/roberta-base-rte\",\n",
    "    # \"JeremiahZ/roberta-base-mnli\",\n",
    "    # \"JeremiahZ/roberta-base-qnli\",\n",
    "    # \"JeremiahZ/roberta-base-sst2\",\n",
    "    # \"JeremiahZ/roberta-base-cola\",\n",
    "    # \"JeremiahZ/roberta-base-mrpc\",\n",
    "    # \"JeremiahZ/roberta-base-qqp\",\n",
    "    # \"JeremiahZ/roberta-base-stsb\",\n",
    "    # \"l-yohai/bigbird-roberta-base-mnli\",\n",
    "    # \"howey/roberta-large-squad2\",\n",
    "]\n",
    "# textattack/roberta-base-RTE ({'accuracy': 0.7}, {'accuracy': 0.7})\n",
    "# textattack/roberta-base-MNLI ({'accuracy': 0.3}, {'accuracy': 0.3})\n",
    "# howey/roberta-large-rte ({'accuracy': 0.65}, {'accuracy': 0.65})\n",
    "# howey/roberta-large-mnli ({'accuracy': 0.68}, {'accuracy': 0.68})\n",
    "# howey/roberta-large-qnli ({'accuracy': 0.86}, {'accuracy': 0.86})\n",
    "# howey/roberta-large-sst2 ({'accuracy': 0.77}, {'accuracy': 0.77})\n",
    "# howey/roberta-large-cola ({'matthews_correlation': 0.19169538058831714}, {'matthews_correlation': 0.19169538058831714})\n",
    "# howey/roberta-large-mrpc ({'accuracy': 0.61, 'f1': 0.6486486486486487}, {'accuracy': 0.61, 'f1': 0.6486486486486487})\n",
    "# howey/roberta-large-qqp ({'accuracy': 0.77, 'f1': 0.5490196078431372}, {'accuracy': 0.77, 'f1': 0.5490196078431372})\n",
    "# JeremiahZ/roberta-base-rte ({'accuracy': 0.61}, {'accuracy': 0.61})\n",
    "# JeremiahZ/roberta-base-mnli ({'accuracy': 0.83}, {'accuracy': 0.83})\n",
    "# JeremiahZ/roberta-base-qnli ({'accuracy': 0.82}, {'accuracy': 0.82})\n",
    "# JeremiahZ/roberta-base-sst2 ({'accuracy': 0.89}, {'accuracy': 0.89})\n",
    "# JeremiahZ/roberta-base-cola ({'matthews_correlation': 0.15285569591066622}, {'matthews_correlation': 0.15285569591066622})\n",
    "# JeremiahZ/roberta-base-mrpc ({'accuracy': 0.33, 'f1': 0.10666666666666666}, {'accuracy': 0.33, 'f1': 0.10666666666666666})\n",
    "# JeremiahZ/roberta-base-qqp ({'accuracy': 0.79, 'f1': 0.7341772151898734}, {'accuracy': 0.79, 'f1': 0.7341772151898734})\n",
    "\n",
    "\n",
    "records = []\n",
    "for model_id in model_ids:\n",
    "    records.append(dict(model_id=model_id, results=str(test_weaver_cached(model_id))))\n",
    "\n",
    "weaving_test_results_df = pd.DataFrame(records)\n",
    "# weaving_test_results_df.to_csv(\"test-weaving-on-base-models.weaving_test_results.csv\")\n",
    "weaving_test_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step get original model baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 1537.71it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1681.76it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1742.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 74.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>split</th>\n",
       "      <th>n_examples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>matthews_correlation</th>\n",
       "      <th>f1</th>\n",
       "      <th>task</th>\n",
       "      <th>roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>JeremiahZ/roberta-base-cola</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JeremiahZ/roberta-base-cola</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.452776</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JeremiahZ/roberta-base-cola</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.172932</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>howey/roberta-large-cola</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>howey/roberta-large-cola</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.451394</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>howey/roberta-large-cola</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.235292</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.296875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339844</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.234375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.960938</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.265625</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.855469</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324219</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.8125</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.707031</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>JeremiahZ/roberta-base-mrpc</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.035156</td>\n",
       "      <td></td>\n",
       "      <td>0.067925</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JeremiahZ/roberta-base-mrpc</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339844</td>\n",
       "      <td></td>\n",
       "      <td>0.076503</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>JeremiahZ/roberta-base-mrpc</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.328125</td>\n",
       "      <td></td>\n",
       "      <td>0.085106</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>howey/roberta-large-mrpc</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.347656</td>\n",
       "      <td></td>\n",
       "      <td>0.515942</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>howey/roberta-large-mrpc</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.632812</td>\n",
       "      <td></td>\n",
       "      <td>0.635659</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>howey/roberta-large-mrpc</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.644531</td>\n",
       "      <td></td>\n",
       "      <td>0.678445</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>JeremiahZ/roberta-base-qnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.492188</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JeremiahZ/roberta-base-qnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.914062</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JeremiahZ/roberta-base-qnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.835938</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>howey/roberta-large-qnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.476562</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>howey/roberta-large-qnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.960938</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>howey/roberta-large-qnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.886719</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>JeremiahZ/roberta-base-qqp</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.292969</td>\n",
       "      <td></td>\n",
       "      <td>0.453172</td>\n",
       "      <td>qqp</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JeremiahZ/roberta-base-qqp</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.875</td>\n",
       "      <td></td>\n",
       "      <td>0.854545</td>\n",
       "      <td>qqp</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>JeremiahZ/roberta-base-qqp</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.847656</td>\n",
       "      <td></td>\n",
       "      <td>0.797927</td>\n",
       "      <td>qqp</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>howey/roberta-large-qqp</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.085938</td>\n",
       "      <td></td>\n",
       "      <td>0.158273</td>\n",
       "      <td>qqp</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>howey/roberta-large-qqp</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.769531</td>\n",
       "      <td></td>\n",
       "      <td>0.598639</td>\n",
       "      <td>qqp</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>howey/roberta-large-qqp</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.78125</td>\n",
       "      <td></td>\n",
       "      <td>0.575758</td>\n",
       "      <td>qqp</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.617188</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.796875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.796875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.660156</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.726562</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.621094</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.25</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.691406</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.644531</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>JeremiahZ/roberta-base-sst2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.433594</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JeremiahZ/roberta-base-sst2</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.738281</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JeremiahZ/roberta-base-sst2</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.867188</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>howey/roberta-large-sst2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.6875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>howey/roberta-large-sst2</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.75</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>howey/roberta-large-sst2</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.828125</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model_id       split  n_examples  accuracy  \\\n",
       "45   JeremiahZ/roberta-base-cola        test         256             \n",
       "13   JeremiahZ/roberta-base-cola       train         256             \n",
       "29   JeremiahZ/roberta-base-cola  validation         256             \n",
       "38      howey/roberta-large-cola        test         256             \n",
       "6       howey/roberta-large-cola       train         256             \n",
       "22      howey/roberta-large-cola  validation         256             \n",
       "33  textattack/roberta-base-MNLI        test         256  0.296875   \n",
       "42   JeremiahZ/roberta-base-mnli        test         256  0.339844   \n",
       "1   textattack/roberta-base-MNLI       train         256  0.234375   \n",
       "10   JeremiahZ/roberta-base-mnli       train         256  0.960938   \n",
       "17  textattack/roberta-base-MNLI  validation         256  0.265625   \n",
       "26   JeremiahZ/roberta-base-mnli  validation         256  0.855469   \n",
       "35      howey/roberta-large-mnli        test         256  0.324219   \n",
       "3       howey/roberta-large-mnli       train         256    0.8125   \n",
       "19      howey/roberta-large-mnli  validation         256  0.707031   \n",
       "46   JeremiahZ/roberta-base-mrpc        test         256  0.035156   \n",
       "14   JeremiahZ/roberta-base-mrpc       train         256  0.339844   \n",
       "30   JeremiahZ/roberta-base-mrpc  validation         256  0.328125   \n",
       "39      howey/roberta-large-mrpc        test         256  0.347656   \n",
       "7       howey/roberta-large-mrpc       train         256  0.632812   \n",
       "23      howey/roberta-large-mrpc  validation         256  0.644531   \n",
       "43   JeremiahZ/roberta-base-qnli        test         256  0.492188   \n",
       "11   JeremiahZ/roberta-base-qnli       train         256  0.914062   \n",
       "27   JeremiahZ/roberta-base-qnli  validation         256  0.835938   \n",
       "36      howey/roberta-large-qnli        test         256  0.476562   \n",
       "4       howey/roberta-large-qnli       train         256  0.960938   \n",
       "20      howey/roberta-large-qnli  validation         256  0.886719   \n",
       "47    JeremiahZ/roberta-base-qqp        test         256  0.292969   \n",
       "15    JeremiahZ/roberta-base-qqp       train         256     0.875   \n",
       "31    JeremiahZ/roberta-base-qqp  validation         256  0.847656   \n",
       "40       howey/roberta-large-qqp        test         256  0.085938   \n",
       "8        howey/roberta-large-qqp       train         256  0.769531   \n",
       "24       howey/roberta-large-qqp  validation         256   0.78125   \n",
       "32   textattack/roberta-base-RTE        test         256  0.617188   \n",
       "41    JeremiahZ/roberta-base-rte        test         256  0.796875   \n",
       "0    textattack/roberta-base-RTE       train         256  0.796875   \n",
       "9     JeremiahZ/roberta-base-rte       train         256  0.660156   \n",
       "16   textattack/roberta-base-RTE  validation         256  0.726562   \n",
       "25    JeremiahZ/roberta-base-rte  validation         256  0.621094   \n",
       "34       howey/roberta-large-rte        test         256      0.25   \n",
       "2        howey/roberta-large-rte       train         256  0.691406   \n",
       "18       howey/roberta-large-rte  validation         256  0.644531   \n",
       "44   JeremiahZ/roberta-base-sst2        test         256  0.433594   \n",
       "12   JeremiahZ/roberta-base-sst2       train         256  0.738281   \n",
       "28   JeremiahZ/roberta-base-sst2  validation         256  0.867188   \n",
       "37      howey/roberta-large-sst2        test         256    0.6875   \n",
       "5       howey/roberta-large-sst2       train         256      0.75   \n",
       "21      howey/roberta-large-sst2  validation         256  0.828125   \n",
       "\n",
       "   matthews_correlation        f1   task roberta  \n",
       "45                  0.0             cola    base  \n",
       "13             0.452776             cola    base  \n",
       "29             0.172932             cola    base  \n",
       "38                  0.0             cola   large  \n",
       "6              0.451394             cola   large  \n",
       "22             0.235292             cola   large  \n",
       "33                                  mnli    base  \n",
       "42                                  mnli    base  \n",
       "1                                   mnli    base  \n",
       "10                                  mnli    base  \n",
       "17                                  mnli    base  \n",
       "26                                  mnli    base  \n",
       "35                                  mnli   large  \n",
       "3                                   mnli   large  \n",
       "19                                  mnli   large  \n",
       "46                       0.067925   mrpc    base  \n",
       "14                       0.076503   mrpc    base  \n",
       "30                       0.085106   mrpc    base  \n",
       "39                       0.515942   mrpc   large  \n",
       "7                        0.635659   mrpc   large  \n",
       "23                       0.678445   mrpc   large  \n",
       "43                                  qnli    base  \n",
       "11                                  qnli    base  \n",
       "27                                  qnli    base  \n",
       "36                                  qnli   large  \n",
       "4                                   qnli   large  \n",
       "20                                  qnli   large  \n",
       "47                       0.453172    qqp    base  \n",
       "15                       0.854545    qqp    base  \n",
       "31                       0.797927    qqp    base  \n",
       "40                       0.158273    qqp   large  \n",
       "8                        0.598639    qqp   large  \n",
       "24                       0.575758    qqp   large  \n",
       "32                                   rte    base  \n",
       "41                                   rte    base  \n",
       "0                                    rte    base  \n",
       "9                                    rte    base  \n",
       "16                                   rte    base  \n",
       "25                                   rte    base  \n",
       "34                                   rte   large  \n",
       "2                                    rte   large  \n",
       "18                                   rte   large  \n",
       "44                                 sst-2    base  \n",
       "12                                 sst-2    base  \n",
       "28                                 sst-2    base  \n",
       "37                                 sst-2   large  \n",
       "5                                  sst-2   large  \n",
       "21                                 sst-2   large  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_weaver import normalize_glue_task_name\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_examples = 256\n",
    "\n",
    "records = []\n",
    "for split in tqdm([\"train\", \"validation\", \"test\"]):\n",
    "    for model_id in tqdm(model_ids):\n",
    "        records.append(\n",
    "            {\n",
    "                \"model_id\": model_id,\n",
    "                \"split\": split,\n",
    "                \"score\": get_score_from_named_model_cached(\n",
    "                    model_id=model_id,\n",
    "                    split=split,\n",
    "                    n_examples=n_examples,\n",
    "                    max_length=128,\n",
    "                    batch_size=128,\n",
    "                ),\n",
    "                \"n_examples\": n_examples,\n",
    "            }\n",
    "        )\n",
    "import pandas as pd\n",
    "\n",
    "# Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.join(pd.json_normalize(df[\"score\"])).drop(columns=[\"score\"])\n",
    "df[\"task\"] = df[\"model_id\"].apply(normalize_glue_task_name)\n",
    "df[\"roberta\"] = df[\"model_id\"].apply(lambda x: \"large\" if \"large\" in x else \"base\")\n",
    "# df = df[df[\"split\"] == \"train\"]\n",
    "# df = df[~df[\"accuracy\"].isna()]\n",
    "df = df.sort_values([\"task\", \"roberta\", \"split\"])\n",
    "# replace nan with ''\n",
    "df = df.fillna(\"\")\n",
    "df.to_csv(\"test-weaving-on-base-models.original-scores.csv\", index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
