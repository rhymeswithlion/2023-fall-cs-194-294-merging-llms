{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoughts on the blank model\n",
    "\n",
    "The weaving-prototype notebook loads the model it will overwrite in the same way it loads the source models.\n",
    "\n",
    "See below how we use the \"textattack/roberta-base-RTE\" architecture as a \"blank\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def get_model_and_tokenizer(identifier):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(identifier)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(identifier, from_pt=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "target_model = get_model_and_tokenizer(\"textattack/roberta-base-RTE\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLaye  multiple                  124055040 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " classifier (TFRobertaClass  multiple                  592130    \n",
      " ificationHead)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124647170 (475.49 MB)\n",
      "Trainable params: 124647170 (475.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show the model architecture\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._1/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._2/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._3/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._4/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._5/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._6/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._7/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._8/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._9/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._10/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._11/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/embeddings/word_embeddings/weight:0 (50265, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/embeddings/token_type_embeddings/embeddings:0 (1, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/embeddings/position_embeddings/embeddings:0 (514, 768)\n",
      "tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification/roberta/embeddings/LayerNorm/beta:0 (768,)\n"
     ]
    }
   ],
   "source": [
    "# Show all the weights, from both the roberta and the classifier part of the model\n",
    "for weight_obj in target_model.roberta.weights:\n",
    "    print(weight_obj.name, weight_obj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(target_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we create a blank model more generally?\n",
    "\n",
    "Take a look at the class's code:\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/2fc33ebead50383f7707b17f0e2a178d86347d10/src/transformers/models/roberta/modeling_tf_roberta.py#L1237-L1246\n",
    "\n",
    "See\n",
    "```\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.roberta = TFRobertaMainLayer(config, add_pooling_layer=False, name=\"roberta\")\n",
    "        self.classifier = TFRobertaClassificationHead(config, name=\"classifier\")\n",
    "```\n",
    "\n",
    "It seems like we might be able to instantiate one with the right value for `config`.\n",
    "\n",
    "For instance, we might be able to specify something other than 12 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"textattack/roberta-base-RTE\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"finetuning_task\": \"glue:rte\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at the config for target_model\n",
    "\n",
    "target_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"awesome_cs194_group/blank-model\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForSequenceClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"finetuning_task\": \"glue:rte\",\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Awesome! These look like the parameters that we'll need, except possibly _name_or_path and maybe architecures\n",
    "# Let's try to create a new model with these parameters\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "\n",
    "new_config_dict = target_model.config.to_dict()\n",
    "\n",
    "new_config_dict[\"_name_or_path\"] = \"awesome_cs194_group/blank-model\"\n",
    "\n",
    "new_config = RobertaConfig(**new_config_dict)\n",
    "new_config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/blank-model-thoughts.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/blank-model-thoughts.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mroberta\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodeling_tf_roberta\u001b[39;00m \u001b[39mimport\u001b[39;00m TFRobertaForSequenceClassification\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/blank-model-thoughts.ipynb#X33sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m blank_model \u001b[39m=\u001b[39m TFRobertaForSequenceClassification(new_config)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/briancruz/2023-fall-cs-194-294-merging-llms/notebooks/blank-model-thoughts.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m blank_model\u001b[39m.\u001b[39;49msummary()\n",
      "File \u001b[0;32m~/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/keras/src/engine/training.py:3403\u001b[0m, in \u001b[0;36mModel.summary\u001b[0;34m(self, line_length, positions, print_fn, expand_nested, show_trainable, layer_range)\u001b[0m\n\u001b[1;32m   3372\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prints a string summary of the network.\u001b[39;00m\n\u001b[1;32m   3373\u001b[0m \n\u001b[1;32m   3374\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3400\u001b[0m \u001b[39m    ValueError: if `summary()` is called before the model is built.\u001b[39;00m\n\u001b[1;32m   3401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3402\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilt:\n\u001b[0;32m-> 3403\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   3404\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3405\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3406\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   3407\u001b[0m     )\n\u001b[1;32m   3408\u001b[0m layer_utils\u001b[39m.\u001b[39mprint_summary(\n\u001b[1;32m   3409\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   3410\u001b[0m     line_length\u001b[39m=\u001b[39mline_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3415\u001b[0m     layer_range\u001b[39m=\u001b[39mlayer_range,\n\u001b[1;32m   3416\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or by calling the model on a batch of data."
     ]
    }
   ],
   "source": [
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "\n",
    "\n",
    "blank_model = TFRobertaForSequenceClassification(new_config)\n",
    "blank_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification at 0x344cff460>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oh, let's run .build first\n",
    "\n",
    "blank_model.build()\n",
    "blank_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._0/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._1/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._2/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._3/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._4/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._5/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._6/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._7/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._8/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._9/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._10/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/encoder/layer_._11/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/embeddings/word_embeddings/weight:0 (50265, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/embeddings/token_type_embeddings/embeddings:0 (1, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/embeddings/position_embeddings/embeddings:0 (514, 768)\n",
      "tf_roberta_for_sequence_classification_6/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/roberta/embeddings/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/classifier/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_6/classifier/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_6/classifier/out_proj/kernel:0 (768, 2)\n",
      "tf_roberta_for_sequence_classification_6/classifier/out_proj/bias:0 (2,)\n"
     ]
    }
   ],
   "source": [
    "for item in blank_model.weights:\n",
    "    print(item.name, item.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!!\n",
    "\n",
    "## Can we create a blank model with a different number of layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLaye  multiple                  145318656 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " classifier (TFRobertaClass  multiple                  592130    \n",
      " ificationHead)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 145910786 (556.61 MB)\n",
      "Trainable params: 145910786 (556.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.roberta.modeling_tf_roberta import TFRobertaForSequenceClassification\n",
    "\n",
    "\n",
    "new_config_dict_15_layer = target_model.config.to_dict()\n",
    "new_config_dict_15_layer[\"num_hidden_layers\"] = 15\n",
    "\n",
    "new_config_15_layer = RobertaConfig(**new_config_dict_15_layer)\n",
    "\n",
    "blank_model_15_layer = TFRobertaForSequenceClassification(new_config_15_layer)\n",
    "blank_model_15_layer.build()\n",
    "blank_model_15_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._0/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._1/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._2/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._3/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._4/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._5/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._6/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._7/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._8/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._9/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._10/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._11/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._12/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._13/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/encoder/layer_._14/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/embeddings/word_embeddings/weight:0 (50265, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/embeddings/token_type_embeddings/embeddings:0 (1, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/embeddings/position_embeddings/embeddings:0 (514, 768)\n",
      "tf_roberta_for_sequence_classification_7/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/roberta/embeddings/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/classifier/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_7/classifier/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_7/classifier/out_proj/kernel:0 (768, 2)\n",
      "tf_roberta_for_sequence_classification_7/classifier/out_proj/bias:0 (2,)\n"
     ]
    }
   ],
   "source": [
    "for item in blank_model_15_layer.weights:\n",
    "    print(item.name, item.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that it? Are we missing something?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
