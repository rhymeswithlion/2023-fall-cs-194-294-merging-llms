{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the `textAttack` models as input models and inspect them\n",
    "\n",
    "Note: We use TFAutoModelForSequenceClassification instead of AutoModelForSequenceClassification because\n",
    "otherwise we get the following message:\n",
    "```python\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"textattack/roberta-base-RTE\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"textattack/roberta-base-RTE\")\n",
    "```\n",
    "```\n",
    "Some weights of the model checkpoint at textattack/roberta-base-RTE were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
    "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
    "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
    "```\n",
    "\n",
    "See https://github.com/huggingface/transformers/blob/2fc33ebead50383f7707b17f0e2a178d86347d10/src/transformers/models/roberta/modeling_roberta.py#L1157-L1246\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_identifers = [\n",
    "    \"textattack/roberta-base-MNLI\",\n",
    "    \"textattack/roberta-base-RTE\"\n",
    "]\n",
    "\n",
    "def get_model_and_tokenizer(identifier):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(identifier)\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(identifier, from_pt=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "input_models = {\n",
    "    identifier: get_model_and_tokenizer(identifier)[0] for identifier in model_identifers\n",
    "}\n",
    "# https://github.com/huggingface/transformers/blob/2fc33ebead50383f7707b17f0e2a178d86347d10/src/transformers/models/roberta/modeling_tf_roberta.py#L1237-L1303\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a target model, i.e. the Blank Model\n",
    "\n",
    "This model will have the final architecture we want to use. This means the classification layer will need to output the number of classes for the task we want to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Instead of using an actual blank model, we'll use \"textattack/roberta-base-RTE\" as our blank model\n",
    "# and overwrite it.\n",
    "target_model = get_model_and_tokenizer(\"textattack/roberta-base-RTE\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLaye  multiple                  124055040 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " classifier (TFRobertaClass  multiple                  592130    \n",
      " ificationHead)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124647170 (475.49 MB)\n",
      "Trainable params: 124647170 (475.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Show the model architecture\n",
    "target_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._1/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._2/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._3/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._4/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._5/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._6/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._7/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._8/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._9/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._10/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/self/query/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/self/query/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/self/key/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/self/key/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/self/value/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/self/value/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/output/dense/kernel:0 (768, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/attention/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/intermediate/dense/kernel:0 (768, 3072)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/intermediate/dense/bias:0 (3072,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/output/dense/kernel:0 (3072, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/output/dense/bias:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/output/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._11/output/LayerNorm/beta:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/embeddings/word_embeddings/weight:0 (50265, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/embeddings/token_type_embeddings/embeddings:0 (1, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/embeddings/position_embeddings/embeddings:0 (514, 768)\n",
      "tf_roberta_for_sequence_classification_2/roberta/embeddings/LayerNorm/gamma:0 (768,)\n",
      "tf_roberta_for_sequence_classification_2/roberta/embeddings/LayerNorm/beta:0 (768,)\n"
     ]
    }
   ],
   "source": [
    "for weight_obj in target_model.roberta.weights:\n",
    "    print(weight_obj.name, weight_obj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the weight sections corresponding to layers by the layer number within a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: 0 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 0 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 0 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 0 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 0 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 0 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 0 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 0 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 0 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 0 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 0 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 0 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 0 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 0 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 0 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 0 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 1 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 1 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 1 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 1 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 1 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 1 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 1 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 1 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 1 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 1 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 1 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 1 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 1 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 1 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 1 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 1 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 2 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 2 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 2 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 2 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 2 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 2 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 2 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 2 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 2 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 2 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 2 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 2 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 2 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 2 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 2 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 2 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 3 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 3 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 3 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 3 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 3 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 3 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 3 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 3 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 3 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 3 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 3 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 3 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 3 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 3 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 3 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 3 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 4 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 4 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 4 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 4 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 4 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 4 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 4 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 4 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 4 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 4 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 4 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 4 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 4 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 4 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 4 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 4 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 5 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 5 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 5 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 5 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 5 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 5 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 5 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 5 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 5 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 5 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 5 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 5 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 5 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 5 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 5 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 5 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 6 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 6 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 6 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 6 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 6 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 6 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 6 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 6 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 6 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 6 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 6 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 6 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 6 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 6 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 6 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 6 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 7 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 7 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 7 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 7 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 7 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 7 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 7 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 7 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 7 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 7 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 7 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 7 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 7 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 7 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 7 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 7 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 8 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 8 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 8 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 8 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 8 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 8 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 8 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 8 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 8 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 8 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 8 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 8 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 8 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 8 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 8 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 8 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 9 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 9 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 9 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 9 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 9 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 9 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 9 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 9 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 9 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 9 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 9 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 9 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 9 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 9 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 9 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 9 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 10 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 10 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 10 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 10 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 10 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 10 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 10 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 10 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 10 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 10 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 10 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 10 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 10 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 10 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 10 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 10 -> weight.name suffix: output/LayerNorm/beta:0\n",
      "Layer: 11 -> weight.name suffix: attention/self/query/kernel:0\n",
      "Layer: 11 -> weight.name suffix: attention/self/query/bias:0\n",
      "Layer: 11 -> weight.name suffix: attention/self/key/kernel:0\n",
      "Layer: 11 -> weight.name suffix: attention/self/key/bias:0\n",
      "Layer: 11 -> weight.name suffix: attention/self/value/kernel:0\n",
      "Layer: 11 -> weight.name suffix: attention/self/value/bias:0\n",
      "Layer: 11 -> weight.name suffix: attention/output/dense/kernel:0\n",
      "Layer: 11 -> weight.name suffix: attention/output/dense/bias:0\n",
      "Layer: 11 -> weight.name suffix: attention/output/LayerNorm/gamma:0\n",
      "Layer: 11 -> weight.name suffix: attention/output/LayerNorm/beta:0\n",
      "Layer: 11 -> weight.name suffix: intermediate/dense/kernel:0\n",
      "Layer: 11 -> weight.name suffix: intermediate/dense/bias:0\n",
      "Layer: 11 -> weight.name suffix: output/dense/kernel:0\n",
      "Layer: 11 -> weight.name suffix: output/dense/bias:0\n",
      "Layer: 11 -> weight.name suffix: output/LayerNorm/gamma:0\n",
      "Layer: 11 -> weight.name suffix: output/LayerNorm/beta:0\n"
     ]
    }
   ],
   "source": [
    "def _get_layer_to_weights_map(model):\n",
    "    from collections import defaultdict\n",
    "    import re\n",
    "\n",
    "    layer_to_weights_map = defaultdict(dict)\n",
    "    for weight in model.weights:\n",
    "        matches = re.findall(r'/layer_._(\\d+)/', weight.name)\n",
    "        if not matches:\n",
    "            continue\n",
    "\n",
    "        layer_number = int(matches[0])\n",
    "        layer_to_weights_map[layer_number][weight.name.partition(f\"/layer_._{layer_number}/\")[-1]] = weight\n",
    "\n",
    "    return {\n",
    "        layer_number: dict(weights)\n",
    "        for layer_number, weights in layer_to_weights_map.items()\n",
    "    }\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for layer_number, weights in _get_layer_to_weights_map(target_model).items():\n",
    "    for weight_name, weight_object in weights.items():\n",
    "        print(f\"Layer: {layer_number} -> weight.name suffix: {weight_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_weights_from_one_layer_to_another(source_model, target_model, source_layer_number, target_layer_number):\n",
    "\n",
    "    # This part is recalculated often, but it's fast. In the future we could \n",
    "    # cache it in a class as a cached property, but we'll leave it here for now.\n",
    "    target_model_layer_to_weights_map = _get_layer_to_weights_map(target_model)\n",
    "    source_model_layer_to_weights_map = _get_layer_to_weights_map(source_model)\n",
    "\n",
    "    # Get the layer objects\n",
    "    source_layer = source_model_layer_to_weights_map[source_layer_number]\n",
    "    target_layer = target_model_layer_to_weights_map[target_layer_number]\n",
    "\n",
    "    # Make sure that all the suffixes match\n",
    "    assert set(source_layer.keys()) == set(target_layer.keys())\n",
    "\n",
    "    # Make sure that all the shapes match\n",
    "    for weight_name, weight_object in source_layer.items():\n",
    "        assert weight_object.shape == target_layer[weight_name].shape\n",
    "    \n",
    "    # Assign weights from one layer to another\n",
    "    for weight_name, weight_object in source_layer.items():\n",
    "        target_layer[weight_name].assign(weight_object.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0\n",
      "[ 0.07277144 -0.00318369 -0.09025939 -0.00312138 -0.09253632  0.18560258\n",
      " -0.04307384  0.02022669  0.00224997  0.06398872]\n"
     ]
    }
   ],
   "source": [
    "# Before assigning\n",
    "print(target_model.weights[0].name)\n",
    "print(target_model.weights[0].numpy()[:10, 0])\n",
    "\n",
    "# tf_roberta_for_sequence_classification_4/roberta/encoder/layer_._0/attention/self/query/kernel:0\n",
    "# [ 0.07277144 -0.00318369 -0.09025939 -0.00312138 -0.09253632  0.18560258\n",
    "#  -0.04307384  0.02022669  0.00224997  0.06398872]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf_roberta_for_sequence_classification/roberta/encoder/layer_._0/attention/self/query/kernel:0\n",
      "[ 0.07214946 -0.00097258 -0.0901752  -0.0041216  -0.09332298  0.18340059\n",
      " -0.04258322  0.0205772  -0.00232515  0.06097307]\n",
      "tf_roberta_for_sequence_classification_2/roberta/encoder/layer_._0/attention/self/query/kernel:0\n",
      "[ 0.07214946 -0.00097258 -0.0901752  -0.0041216  -0.09332298  0.18340059\n",
      " -0.04258322  0.0205772  -0.00232515  0.06097307]\n"
     ]
    }
   ],
   "source": [
    "source_model = input_models[\"textattack/roberta-base-MNLI\"]\n",
    "target_model = target_model\n",
    "\n",
    "assign_weights_from_one_layer_to_another(\n",
    "    source_model=source_model,\n",
    "    target_model=target_model,\n",
    "    source_layer_number=0,\n",
    "    target_layer_number=0\n",
    ")\n",
    "\n",
    "# Where we copied from\n",
    "print(source_model.weights[0].name)\n",
    "print(source_model.weights[0].numpy()[:10, 0])\n",
    "\n",
    "# After assigning\n",
    "print(target_model.weights[0].name)\n",
    "print(target_model.weights[0].numpy()[:10, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a configuration for the target model using the donor models\n",
    "\n",
    "We can extend the configuration to accept more than just \"SingleLayer\" layer assignment types. We can also add Isometric, Fisher, etc. types that incorporate the weights from multiple layers and put them into a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target_model_template': 'textattack/roberta-base-RTE',\n",
       " 'layer_assignments': [{'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 0}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 1}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 2}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 3}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 4}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 5}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 6}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 7}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 8}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 9}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "    'hidden_layer_number': 10}},\n",
       "  {'type': 'SingleLayer',\n",
       "   'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "    'hidden_layer_number': 11}}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a config for a perfect weave, i.e. ABABABABABAB\n",
    "model_weaving_config = {\n",
    "    # The task (i.e. the classification head should match the task at hand)\n",
    "    \"target_model_template\": \"textattack/roberta-base-RTE\",\n",
    "    # layer assignments\n",
    "    \"layer_assignments\": [\n",
    "        {\n",
    "            \"type\": \"SingleLayer\",\n",
    "            \"params\": {\n",
    "                \"donor\": \"textattack/roberta-base-MNLI\" if (i % 2 == 0) else \"textattack/roberta-base-RTE\",\n",
    "                \"hidden_layer_number\": i\n",
    "            },\n",
    "        } for i in range(12)\n",
    "    ],\n",
    "}\n",
    "\n",
    "model_weaving_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weave_models(target_model_template, layer_assignments):\n",
    "    target_model, target_tokenizer = get_model_and_tokenizer(target_model_template)\n",
    "\n",
    "    source_model_names = set(\n",
    "        layer_assignment[\"params\"][\"donor\"]\n",
    "        for layer_assignment in layer_assignments\n",
    "    )\n",
    "    source_models = {\n",
    "        source_model_name: get_model_and_tokenizer(source_model_name)[0]\n",
    "        for source_model_name in source_model_names\n",
    "    }\n",
    "\n",
    "    for layer_assignment in layer_assignments:\n",
    "        if layer_assignment[\"type\"] == \"SingleLayer\":\n",
    "            assign_weights_from_one_layer_to_another(\n",
    "                source_model=source_models[layer_assignment[\"params\"][\"donor\"]],\n",
    "                target_model=target_model,\n",
    "                source_layer_number=layer_assignment[\"params\"][\"hidden_layer_number\"],\n",
    "                target_layer_number=layer_assignment[\"params\"][\"hidden_layer_number\"]\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Unknown layer assignment type: {layer_assignment['type']}\")\n",
    "\n",
    "    return target_model, target_tokenizer\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "weaved_model, weaved_model_tokenizer = weave_models(**model_weaving_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLaye  multiple                  124055040 \n",
      " r)                                                              \n",
      "                                                                 \n",
      " classifier (TFRobertaClass  multiple                  592130    \n",
      " ificationHead)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124647170 (475.49 MB)\n",
      "Trainable params: 124647170 (475.49 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "weaved_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weaved_model.save_pretrained(\"data/weaved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476M\t./data/weaved_model\n"
     ]
    }
   ],
   "source": [
    "! du -h ./data/weaved_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the weaved the model's performance\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
