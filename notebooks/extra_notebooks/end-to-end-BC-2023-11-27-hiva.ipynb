{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end notebook!\n",
    "\n",
    "Here is the workflow:\n",
    "* Sampling configs (sampling parameters, etc.) lead to...\n",
    "* Weaving configs (blank model settings, donor model settings, layer assignments) lead to...\n",
    "* Models (probably TFRobertaForSequenceClassification in all cases) lead to...\n",
    "* Performance scores (numbers from 0-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: visions 0.7.5 does not provide the extra 'type-image-path'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "\n",
    "! pip install -q joblib  # joblib for memoizing functions\n",
    "! pip install -q ipywidgets widgetsnbextension pandas-profiling # IProgress for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model_merging to the python path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "model_merging_base = os.path.abspath(\"../model_merging/\")\n",
    "# assert it exist\n",
    "assert os.path.exists(model_merging_base)\n",
    "if model_merging_base not in sys.path:\n",
    "    sys.path.append(model_merging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib for caching and distributed computing\n",
    "from math import sqrt\n",
    "\n",
    "from joblib import Memory, Parallel, delayed\n",
    "\n",
    "memory = Memory(location=\"cache\", verbose=10)\n",
    "\n",
    "parallel = Parallel(n_jobs=2, return_as=\"generator\")\n",
    "output_generator = parallel(delayed(sqrt)(i**2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and cached functions\n",
    "\n",
    "import os\n",
    "\n",
    "from llm_weaver import (\n",
    "    calculate_score_from_weaving_config,\n",
    "    test_weaver,\n",
    ")\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "calculate_score_from_weaving_config_cached = memory.cache(\n",
    "    calculate_score_from_weaving_config\n",
    ")\n",
    "test_weaver_cached = memory.cache(test_weaver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Test weaving code\n",
    "\n",
    "This test makes sure that our score when using the weaver to reconstruct a model from all its parts get the same evaluation score as the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MemorizedFunc(func=<function test_weaver at 0x2c98a9790>, location=cache/joblib)]: Clearing function cache identified by llm_weaver/test_weaver\n",
      "[MemorizedFunc(func=<function test_weaver at 0x2c71c5790>, location=cache/joblib)]: Clearing function cache identified by llm_weaver/test_weaver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.test_weaver...\n",
      "test_weaver('textattack/roberta-base-MNLI')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.test_weaver...\n",
      "test_weaver('textattack/roberta-base-RTE')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 756986b041cd5656d4878287e6fef62f\n",
      "calculating score for weaving config md5sum: 367fed3b6bf1fd31867ef1207a026479\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-28 08:18:51.141035: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 08:18:51.198737: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 3f302d2841d5e9508a8e33464b313985\n",
      "calculating score for weaving config md5sum: c87dd0438cfc04d8acd9371cc8fb05f9\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 08:19:03.146687: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 08:19:03.291332: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 64b8d72806f9123d7a952fb0df71b630\n",
      "calculating score for weaving config md5sum: 2de68b10f030533e464532058b3b6081\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 08:19:13.745087: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 08:19:13.863533: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-28 08:19:22.902994: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-28 08:19:23.311376: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Linear combo weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Fisher weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "_____________________________________________________test_weaver - 46.0s, 0.8min\n",
      "Original score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "Weaved score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "Linear combo weaved score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "Fisher weaved score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "_____________________________________________________test_weaver - 46.4s, 0.8min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[([{'accuracy': 0.7}, {'accuracy': 0.7}, {'accuracy': 0.7}, {'accuracy': 0.7}],\n",
       "  [None,\n",
       "   {'glue_task': 'rte',\n",
       "    'tokenizer_model_id': 'textattack/roberta-base-RTE',\n",
       "    'blank_model_config': {'return_dict': True,\n",
       "     'output_hidden_states': False,\n",
       "     'output_attentions': False,\n",
       "     'torchscript': False,\n",
       "     'torch_dtype': None,\n",
       "     'use_bfloat16': False,\n",
       "     'tf_legacy_loss': False,\n",
       "     'pruned_heads': {},\n",
       "     'tie_word_embeddings': True,\n",
       "     'is_encoder_decoder': False,\n",
       "     'is_decoder': False,\n",
       "     'cross_attention_hidden_size': None,\n",
       "     'add_cross_attention': False,\n",
       "     'tie_encoder_decoder': False,\n",
       "     'max_length': 20,\n",
       "     'min_length': 0,\n",
       "     'do_sample': False,\n",
       "     'early_stopping': False,\n",
       "     'num_beams': 1,\n",
       "     'num_beam_groups': 1,\n",
       "     'diversity_penalty': 0.0,\n",
       "     'temperature': 1.0,\n",
       "     'top_k': 50,\n",
       "     'top_p': 1.0,\n",
       "     'typical_p': 1.0,\n",
       "     'repetition_penalty': 1.0,\n",
       "     'length_penalty': 1.0,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'encoder_no_repeat_ngram_size': 0,\n",
       "     'bad_words_ids': None,\n",
       "     'num_return_sequences': 1,\n",
       "     'chunk_size_feed_forward': 0,\n",
       "     'output_scores': False,\n",
       "     'return_dict_in_generate': False,\n",
       "     'forced_bos_token_id': None,\n",
       "     'forced_eos_token_id': None,\n",
       "     'remove_invalid_values': False,\n",
       "     'exponential_decay_length_penalty': None,\n",
       "     'suppress_tokens': None,\n",
       "     'begin_suppress_tokens': None,\n",
       "     'architectures': ['RobertaForSequenceClassification'],\n",
       "     'finetuning_task': 'glue:rte',\n",
       "     'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "     'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "     'tokenizer_class': None,\n",
       "     'prefix': None,\n",
       "     'bos_token_id': 0,\n",
       "     'pad_token_id': 1,\n",
       "     'eos_token_id': 2,\n",
       "     'sep_token_id': None,\n",
       "     'decoder_start_token_id': None,\n",
       "     'task_specific_params': None,\n",
       "     'problem_type': None,\n",
       "     'transformers_version': '4.35.0',\n",
       "     'gradient_checkpointing': False,\n",
       "     'model_type': 'roberta',\n",
       "     'vocab_size': 50265,\n",
       "     'hidden_size': 768,\n",
       "     'num_hidden_layers': 12,\n",
       "     'num_attention_heads': 12,\n",
       "     'hidden_act': 'gelu',\n",
       "     'intermediate_size': 3072,\n",
       "     'hidden_dropout_prob': 0.1,\n",
       "     'attention_probs_dropout_prob': 0.1,\n",
       "     'max_position_embeddings': 514,\n",
       "     'type_vocab_size': 1,\n",
       "     'initializer_range': 0.02,\n",
       "     'layer_norm_eps': 1e-05,\n",
       "     'position_embedding_type': 'absolute',\n",
       "     'use_cache': True,\n",
       "     'classifier_dropout': None},\n",
       "    'layer_assignments': [{'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 0}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 1}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 2}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 3}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 4}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 5}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 6}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 7}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 8}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 9}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 10}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-RTE',\n",
       "       'hidden_layer_number': 11}}],\n",
       "    'classification_head': {'type': 'SingleClassificationHead',\n",
       "     'params': {'donor': 'textattack/roberta-base-RTE'}},\n",
       "    'embeddings': {'type': 'SingleEmbeddings',\n",
       "     'params': {'donor': 'textattack/roberta-base-RTE'}}},\n",
       "   {'glue_task': 'rte',\n",
       "    'tokenizer_model_id': 'textattack/roberta-base-RTE',\n",
       "    'blank_model_config': {'return_dict': True,\n",
       "     'output_hidden_states': False,\n",
       "     'output_attentions': False,\n",
       "     'torchscript': False,\n",
       "     'torch_dtype': None,\n",
       "     'use_bfloat16': False,\n",
       "     'tf_legacy_loss': False,\n",
       "     'pruned_heads': {},\n",
       "     'tie_word_embeddings': True,\n",
       "     'is_encoder_decoder': False,\n",
       "     'is_decoder': False,\n",
       "     'cross_attention_hidden_size': None,\n",
       "     'add_cross_attention': False,\n",
       "     'tie_encoder_decoder': False,\n",
       "     'max_length': 20,\n",
       "     'min_length': 0,\n",
       "     'do_sample': False,\n",
       "     'early_stopping': False,\n",
       "     'num_beams': 1,\n",
       "     'num_beam_groups': 1,\n",
       "     'diversity_penalty': 0.0,\n",
       "     'temperature': 1.0,\n",
       "     'top_k': 50,\n",
       "     'top_p': 1.0,\n",
       "     'typical_p': 1.0,\n",
       "     'repetition_penalty': 1.0,\n",
       "     'length_penalty': 1.0,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'encoder_no_repeat_ngram_size': 0,\n",
       "     'bad_words_ids': None,\n",
       "     'num_return_sequences': 1,\n",
       "     'chunk_size_feed_forward': 0,\n",
       "     'output_scores': False,\n",
       "     'return_dict_in_generate': False,\n",
       "     'forced_bos_token_id': None,\n",
       "     'forced_eos_token_id': None,\n",
       "     'remove_invalid_values': False,\n",
       "     'exponential_decay_length_penalty': None,\n",
       "     'suppress_tokens': None,\n",
       "     'begin_suppress_tokens': None,\n",
       "     'architectures': ['RobertaForSequenceClassification'],\n",
       "     'finetuning_task': 'glue:rte',\n",
       "     'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "     'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "     'tokenizer_class': None,\n",
       "     'prefix': None,\n",
       "     'bos_token_id': 0,\n",
       "     'pad_token_id': 1,\n",
       "     'eos_token_id': 2,\n",
       "     'sep_token_id': None,\n",
       "     'decoder_start_token_id': None,\n",
       "     'task_specific_params': None,\n",
       "     'problem_type': None,\n",
       "     'transformers_version': '4.35.0',\n",
       "     'gradient_checkpointing': False,\n",
       "     'model_type': 'roberta',\n",
       "     'vocab_size': 50265,\n",
       "     'hidden_size': 768,\n",
       "     'num_hidden_layers': 12,\n",
       "     'num_attention_heads': 12,\n",
       "     'hidden_act': 'gelu',\n",
       "     'intermediate_size': 3072,\n",
       "     'hidden_dropout_prob': 0.1,\n",
       "     'attention_probs_dropout_prob': 0.1,\n",
       "     'max_position_embeddings': 514,\n",
       "     'type_vocab_size': 1,\n",
       "     'initializer_range': 0.02,\n",
       "     'layer_norm_eps': 1e-05,\n",
       "     'position_embedding_type': 'absolute',\n",
       "     'use_cache': True,\n",
       "     'classifier_dropout': None},\n",
       "    'layer_assignments': [{'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.4}]}}],\n",
       "    'classification_head': {'type': 'SingleClassificationHead',\n",
       "     'params': {'donor': 'textattack/roberta-base-RTE'}},\n",
       "    'embeddings': {'type': 'SingleEmbeddings',\n",
       "     'params': {'donor': 'textattack/roberta-base-RTE'}}},\n",
       "   {'glue_task': 'rte',\n",
       "    'tokenizer_model_id': 'textattack/roberta-base-RTE',\n",
       "    'blank_model_config': {'return_dict': True,\n",
       "     'output_hidden_states': False,\n",
       "     'output_attentions': False,\n",
       "     'torchscript': False,\n",
       "     'torch_dtype': None,\n",
       "     'use_bfloat16': False,\n",
       "     'tf_legacy_loss': False,\n",
       "     'pruned_heads': {},\n",
       "     'tie_word_embeddings': True,\n",
       "     'is_encoder_decoder': False,\n",
       "     'is_decoder': False,\n",
       "     'cross_attention_hidden_size': None,\n",
       "     'add_cross_attention': False,\n",
       "     'tie_encoder_decoder': False,\n",
       "     'max_length': 20,\n",
       "     'min_length': 0,\n",
       "     'do_sample': False,\n",
       "     'early_stopping': False,\n",
       "     'num_beams': 1,\n",
       "     'num_beam_groups': 1,\n",
       "     'diversity_penalty': 0.0,\n",
       "     'temperature': 1.0,\n",
       "     'top_k': 50,\n",
       "     'top_p': 1.0,\n",
       "     'typical_p': 1.0,\n",
       "     'repetition_penalty': 1.0,\n",
       "     'length_penalty': 1.0,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'encoder_no_repeat_ngram_size': 0,\n",
       "     'bad_words_ids': None,\n",
       "     'num_return_sequences': 1,\n",
       "     'chunk_size_feed_forward': 0,\n",
       "     'output_scores': False,\n",
       "     'return_dict_in_generate': False,\n",
       "     'forced_bos_token_id': None,\n",
       "     'forced_eos_token_id': None,\n",
       "     'remove_invalid_values': False,\n",
       "     'exponential_decay_length_penalty': None,\n",
       "     'suppress_tokens': None,\n",
       "     'begin_suppress_tokens': None,\n",
       "     'architectures': ['RobertaForSequenceClassification'],\n",
       "     'finetuning_task': 'glue:rte',\n",
       "     'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
       "     'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
       "     'tokenizer_class': None,\n",
       "     'prefix': None,\n",
       "     'bos_token_id': 0,\n",
       "     'pad_token_id': 1,\n",
       "     'eos_token_id': 2,\n",
       "     'sep_token_id': None,\n",
       "     'decoder_start_token_id': None,\n",
       "     'task_specific_params': None,\n",
       "     'problem_type': None,\n",
       "     'transformers_version': '4.35.0',\n",
       "     'gradient_checkpointing': False,\n",
       "     'model_type': 'roberta',\n",
       "     'vocab_size': 50265,\n",
       "     'hidden_size': 768,\n",
       "     'num_hidden_layers': 12,\n",
       "     'num_attention_heads': 12,\n",
       "     'hidden_act': 'gelu',\n",
       "     'intermediate_size': 3072,\n",
       "     'hidden_dropout_prob': 0.1,\n",
       "     'attention_probs_dropout_prob': 0.1,\n",
       "     'max_position_embeddings': 514,\n",
       "     'type_vocab_size': 1,\n",
       "     'initializer_range': 0.02,\n",
       "     'layer_norm_eps': 1e-05,\n",
       "     'position_embedding_type': 'absolute',\n",
       "     'use_cache': True,\n",
       "     'classifier_dropout': None},\n",
       "    'layer_assignments': [{'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-RTE',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5'}],\n",
       "       'normalize': True}}],\n",
       "    'classification_head': {'type': 'SingleClassificationHead',\n",
       "     'params': {'donor': 'textattack/roberta-base-RTE'}},\n",
       "    'embeddings': {'type': 'SingleEmbeddings',\n",
       "     'params': {'donor': 'textattack/roberta-base-RTE'}}}]),\n",
       " ([{'accuracy': 0.3}, {'accuracy': 0.3}, {'accuracy': 0.3}, {'accuracy': 0.3}],\n",
       "  [None,\n",
       "   {'glue_task': 'mnli',\n",
       "    'tokenizer_model_id': 'textattack/roberta-base-MNLI',\n",
       "    'blank_model_config': {'return_dict': True,\n",
       "     'output_hidden_states': False,\n",
       "     'output_attentions': False,\n",
       "     'torchscript': False,\n",
       "     'torch_dtype': None,\n",
       "     'use_bfloat16': False,\n",
       "     'tf_legacy_loss': False,\n",
       "     'pruned_heads': {},\n",
       "     'tie_word_embeddings': True,\n",
       "     'is_encoder_decoder': False,\n",
       "     'is_decoder': False,\n",
       "     'cross_attention_hidden_size': None,\n",
       "     'add_cross_attention': False,\n",
       "     'tie_encoder_decoder': False,\n",
       "     'max_length': 20,\n",
       "     'min_length': 0,\n",
       "     'do_sample': False,\n",
       "     'early_stopping': False,\n",
       "     'num_beams': 1,\n",
       "     'num_beam_groups': 1,\n",
       "     'diversity_penalty': 0.0,\n",
       "     'temperature': 1.0,\n",
       "     'top_k': 50,\n",
       "     'top_p': 1.0,\n",
       "     'typical_p': 1.0,\n",
       "     'repetition_penalty': 1.0,\n",
       "     'length_penalty': 1.0,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'encoder_no_repeat_ngram_size': 0,\n",
       "     'bad_words_ids': None,\n",
       "     'num_return_sequences': 1,\n",
       "     'chunk_size_feed_forward': 0,\n",
       "     'output_scores': False,\n",
       "     'return_dict_in_generate': False,\n",
       "     'forced_bos_token_id': None,\n",
       "     'forced_eos_token_id': None,\n",
       "     'remove_invalid_values': False,\n",
       "     'exponential_decay_length_penalty': None,\n",
       "     'suppress_tokens': None,\n",
       "     'begin_suppress_tokens': None,\n",
       "     'architectures': ['RobertaForSequenceClassification'],\n",
       "     'finetuning_task': 'mnli',\n",
       "     'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n",
       "     'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n",
       "     'tokenizer_class': None,\n",
       "     'prefix': None,\n",
       "     'bos_token_id': 0,\n",
       "     'pad_token_id': 1,\n",
       "     'eos_token_id': 2,\n",
       "     'sep_token_id': None,\n",
       "     'decoder_start_token_id': None,\n",
       "     'task_specific_params': None,\n",
       "     'problem_type': None,\n",
       "     'transformers_version': '4.35.0',\n",
       "     'model_type': 'roberta',\n",
       "     'vocab_size': 50265,\n",
       "     'hidden_size': 768,\n",
       "     'num_hidden_layers': 12,\n",
       "     'num_attention_heads': 12,\n",
       "     'hidden_act': 'gelu',\n",
       "     'intermediate_size': 3072,\n",
       "     'hidden_dropout_prob': 0.1,\n",
       "     'attention_probs_dropout_prob': 0.1,\n",
       "     'max_position_embeddings': 514,\n",
       "     'type_vocab_size': 1,\n",
       "     'initializer_range': 0.02,\n",
       "     'layer_norm_eps': 1e-05,\n",
       "     'position_embedding_type': 'absolute',\n",
       "     'use_cache': True,\n",
       "     'classifier_dropout': None},\n",
       "    'layer_assignments': [{'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 0}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 1}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 2}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 3}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 4}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 5}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 6}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 7}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 8}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 9}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 10}},\n",
       "     {'type': 'SingleLayer',\n",
       "      'params': {'donor': 'textattack/roberta-base-MNLI',\n",
       "       'hidden_layer_number': 11}}],\n",
       "    'classification_head': {'type': 'SingleClassificationHead',\n",
       "     'params': {'donor': 'textattack/roberta-base-MNLI'}},\n",
       "    'embeddings': {'type': 'SingleEmbeddings',\n",
       "     'params': {'donor': 'textattack/roberta-base-MNLI'}}},\n",
       "   {'glue_task': 'mnli',\n",
       "    'tokenizer_model_id': 'textattack/roberta-base-MNLI',\n",
       "    'blank_model_config': {'return_dict': True,\n",
       "     'output_hidden_states': False,\n",
       "     'output_attentions': False,\n",
       "     'torchscript': False,\n",
       "     'torch_dtype': None,\n",
       "     'use_bfloat16': False,\n",
       "     'tf_legacy_loss': False,\n",
       "     'pruned_heads': {},\n",
       "     'tie_word_embeddings': True,\n",
       "     'is_encoder_decoder': False,\n",
       "     'is_decoder': False,\n",
       "     'cross_attention_hidden_size': None,\n",
       "     'add_cross_attention': False,\n",
       "     'tie_encoder_decoder': False,\n",
       "     'max_length': 20,\n",
       "     'min_length': 0,\n",
       "     'do_sample': False,\n",
       "     'early_stopping': False,\n",
       "     'num_beams': 1,\n",
       "     'num_beam_groups': 1,\n",
       "     'diversity_penalty': 0.0,\n",
       "     'temperature': 1.0,\n",
       "     'top_k': 50,\n",
       "     'top_p': 1.0,\n",
       "     'typical_p': 1.0,\n",
       "     'repetition_penalty': 1.0,\n",
       "     'length_penalty': 1.0,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'encoder_no_repeat_ngram_size': 0,\n",
       "     'bad_words_ids': None,\n",
       "     'num_return_sequences': 1,\n",
       "     'chunk_size_feed_forward': 0,\n",
       "     'output_scores': False,\n",
       "     'return_dict_in_generate': False,\n",
       "     'forced_bos_token_id': None,\n",
       "     'forced_eos_token_id': None,\n",
       "     'remove_invalid_values': False,\n",
       "     'exponential_decay_length_penalty': None,\n",
       "     'suppress_tokens': None,\n",
       "     'begin_suppress_tokens': None,\n",
       "     'architectures': ['RobertaForSequenceClassification'],\n",
       "     'finetuning_task': 'mnli',\n",
       "     'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n",
       "     'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n",
       "     'tokenizer_class': None,\n",
       "     'prefix': None,\n",
       "     'bos_token_id': 0,\n",
       "     'pad_token_id': 1,\n",
       "     'eos_token_id': 2,\n",
       "     'sep_token_id': None,\n",
       "     'decoder_start_token_id': None,\n",
       "     'task_specific_params': None,\n",
       "     'problem_type': None,\n",
       "     'transformers_version': '4.35.0',\n",
       "     'model_type': 'roberta',\n",
       "     'vocab_size': 50265,\n",
       "     'hidden_size': 768,\n",
       "     'num_hidden_layers': 12,\n",
       "     'num_attention_heads': 12,\n",
       "     'hidden_act': 'gelu',\n",
       "     'intermediate_size': 3072,\n",
       "     'hidden_dropout_prob': 0.1,\n",
       "     'attention_probs_dropout_prob': 0.1,\n",
       "     'max_position_embeddings': 514,\n",
       "     'type_vocab_size': 1,\n",
       "     'initializer_range': 0.02,\n",
       "     'layer_norm_eps': 1e-05,\n",
       "     'position_embedding_type': 'absolute',\n",
       "     'use_cache': True,\n",
       "     'classifier_dropout': None},\n",
       "    'layer_assignments': [{'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 0.4}]}},\n",
       "     {'type': 'IsotropicLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.1},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.2},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.30000000000000004},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 0.4}]}}],\n",
       "    'classification_head': {'type': 'SingleClassificationHead',\n",
       "     'params': {'donor': 'textattack/roberta-base-MNLI'}},\n",
       "    'embeddings': {'type': 'SingleEmbeddings',\n",
       "     'params': {'donor': 'textattack/roberta-base-MNLI'}}},\n",
       "   {'glue_task': 'mnli',\n",
       "    'tokenizer_model_id': 'textattack/roberta-base-MNLI',\n",
       "    'blank_model_config': {'return_dict': True,\n",
       "     'output_hidden_states': False,\n",
       "     'output_attentions': False,\n",
       "     'torchscript': False,\n",
       "     'torch_dtype': None,\n",
       "     'use_bfloat16': False,\n",
       "     'tf_legacy_loss': False,\n",
       "     'pruned_heads': {},\n",
       "     'tie_word_embeddings': True,\n",
       "     'is_encoder_decoder': False,\n",
       "     'is_decoder': False,\n",
       "     'cross_attention_hidden_size': None,\n",
       "     'add_cross_attention': False,\n",
       "     'tie_encoder_decoder': False,\n",
       "     'max_length': 20,\n",
       "     'min_length': 0,\n",
       "     'do_sample': False,\n",
       "     'early_stopping': False,\n",
       "     'num_beams': 1,\n",
       "     'num_beam_groups': 1,\n",
       "     'diversity_penalty': 0.0,\n",
       "     'temperature': 1.0,\n",
       "     'top_k': 50,\n",
       "     'top_p': 1.0,\n",
       "     'typical_p': 1.0,\n",
       "     'repetition_penalty': 1.0,\n",
       "     'length_penalty': 1.0,\n",
       "     'no_repeat_ngram_size': 0,\n",
       "     'encoder_no_repeat_ngram_size': 0,\n",
       "     'bad_words_ids': None,\n",
       "     'num_return_sequences': 1,\n",
       "     'chunk_size_feed_forward': 0,\n",
       "     'output_scores': False,\n",
       "     'return_dict_in_generate': False,\n",
       "     'forced_bos_token_id': None,\n",
       "     'forced_eos_token_id': None,\n",
       "     'remove_invalid_values': False,\n",
       "     'exponential_decay_length_penalty': None,\n",
       "     'suppress_tokens': None,\n",
       "     'begin_suppress_tokens': None,\n",
       "     'architectures': ['RobertaForSequenceClassification'],\n",
       "     'finetuning_task': 'mnli',\n",
       "     'id2label': {0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2'},\n",
       "     'label2id': {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2},\n",
       "     'tokenizer_class': None,\n",
       "     'prefix': None,\n",
       "     'bos_token_id': 0,\n",
       "     'pad_token_id': 1,\n",
       "     'eos_token_id': 2,\n",
       "     'sep_token_id': None,\n",
       "     'decoder_start_token_id': None,\n",
       "     'task_specific_params': None,\n",
       "     'problem_type': None,\n",
       "     'transformers_version': '4.35.0',\n",
       "     'model_type': 'roberta',\n",
       "     'vocab_size': 50265,\n",
       "     'hidden_size': 768,\n",
       "     'num_hidden_layers': 12,\n",
       "     'num_attention_heads': 12,\n",
       "     'hidden_act': 'gelu',\n",
       "     'intermediate_size': 3072,\n",
       "     'hidden_dropout_prob': 0.1,\n",
       "     'attention_probs_dropout_prob': 0.1,\n",
       "     'max_position_embeddings': 514,\n",
       "     'type_vocab_size': 1,\n",
       "     'initializer_range': 0.02,\n",
       "     'layer_norm_eps': 1e-05,\n",
       "     'position_embedding_type': 'absolute',\n",
       "     'use_cache': True,\n",
       "     'classifier_dropout': None},\n",
       "    'layer_assignments': [{'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 0,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 1,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 2,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 3,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 4,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 5,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 6,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 7,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 8,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 9,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 10,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}},\n",
       "     {'type': 'ElementWiseLinearCombination',\n",
       "      'params': {'donors': [{'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': None},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': 1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'},\n",
       "        {'donor': 'textattack/roberta-base-MNLI',\n",
       "         'hidden_layer_number': 11,\n",
       "         'weight': -1.0,\n",
       "         'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-MNLI-fisher-info.h5'}],\n",
       "       'normalize': True}}],\n",
       "    'classification_head': {'type': 'SingleClassificationHead',\n",
       "     'params': {'donor': 'textattack/roberta-base-MNLI'}},\n",
       "    'embeddings': {'type': 'SingleEmbeddings',\n",
       "     'params': {'donor': 'textattack/roberta-base-MNLI'}}}])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids = [\n",
    "    \"textattack/roberta-base-RTE\",\n",
    "    \"textattack/roberta-base-MNLI\",\n",
    "]\n",
    "\n",
    "# for model_id in model_ids:\n",
    "#     test_weaver(model_id)\n",
    "\n",
    "# You can run this more than once, and it will pull from the cache on subsequent runs\n",
    "Parallel(n_jobs=2, return_as=\"list\")(\n",
    "    delayed(test_weaver_cached)(model_id) for model_id in model_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps 1-3: configs to graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to sample configs\n",
    "import random\n",
    "\n",
    "from llm_weaver import dict_overwrite, get_model_config\n",
    "\n",
    "\n",
    "def sample_weave_configs_iter(p=0.5, seed=42, max_configs=1):\n",
    "    # set random seed\n",
    "    random.seed(seed)\n",
    "\n",
    "    donor_model_ids = [\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "        \"textattack/roberta-base-MNLI\",\n",
    "    ]\n",
    "    blank_model_config = dict_overwrite(\n",
    "        get_model_config(\"textattack/roberta-base-RTE\"),\n",
    "        {\n",
    "            \"num_hidden_layers\": 12 ,\n",
    "        },\n",
    "    )\n",
    "    for _ in range(max_configs):\n",
    "        config = {\n",
    "            \"glue_task\": \"rte\",\n",
    "            \"tokenizer_model_id\": \"textattack/roberta-base-RTE\",\n",
    "            # The task (i.e. the classification head output size should match the task at hand)\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            # Layer assignments\n",
    "            \"layer_assignments\": [\n",
    "                {\n",
    "                    \"type\": \"SingleLayer\",\n",
    "                    \"params\": {\n",
    "                        # Load donor model # Choose a random donor model according to the p parameter\n",
    "                        \"donor\": random.choices(donor_model_ids, weights=[p, 1 - p])[0],\n",
    "                        # Pick a layer\n",
    "                        \"hidden_layer_number\": i,\n",
    "                    },\n",
    "                }\n",
    "                for i in range(12)\n",
    "            ],\n",
    "            # The head (i.e. the classification head should match the task at hand)\n",
    "            # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                },\n",
    "            },\n",
    "            # The embeddings layer\n",
    "            # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "\n",
    "        yield config\n",
    "\n",
    "\n",
    "sample_config = dict(p=0.5, seed=42, max_configs=100)\n",
    "\n",
    "# Generate the sample configs and save to a file just in case\n",
    "weave_configs = list(sample_weave_configs_iter(**sample_config))\n",
    "\n",
    "\n",
    "len(weave_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 98361db8aed5fbcc51290cd4edcb0f57\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 1bc4086f7f1983092218412b60cadc87\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 5e1c00a997b1e4832e75b034c6913906\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: ab200dbe24c8f9ab8c9c504f00ad96b8\n",
      "calculating score for weaving config md5sum: 3460118f9a95138cbcfe79463717d75b\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 61.7s, 1.0min\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 64.6s, 1.1min\n",
      "_____________________________calculate_score_from_weaving_config - 64.8s, 1.1min\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 64.9s, 1.1min\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 65.1s, 1.1min\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sample_config_to_plots(sample_config, n_jobs=5):\n",
    "    weave_configs = list(\n",
    "        sample_weave_configs_iter(**sample_config),\n",
    "    )\n",
    "\n",
    "    scores = Parallel(n_jobs=n_jobs, return_as=\"list\")(\n",
    "        delayed(calculate_score_from_weaving_config_cached)(\n",
    "            weave_config,\n",
    "            n_examples=4096,\n",
    "            split=\"validation\",\n",
    "        )\n",
    "        for weave_config in weave_configs\n",
    "    )\n",
    "    accuracies = [score[\"accuracy\"] for score in scores]\n",
    "\n",
    "    title = f\"Accuracy distribution on task {weave_configs[0]['glue_task']} with p={sample_config['p']} with N={len(accuracies)}\"\n",
    "\n",
    "    # create figure and ax\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(accuracies, bins=10)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return accuracies, weave_configs\n",
    "\n",
    "\n",
    "accuracies, weave_configs = sample_config_to_plots(\n",
    "    dict(p=0.5, seed=42, max_configs=5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1bec1a39a966bfc720c35cd575945ee7\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1a9cf0cf45c1a6c0d475f9baf72bc8e6\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/081e27a9c4ade95b002fa3f8f3d523a9\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/5a6a8057fb8ccaf3ec76155940ce57eb\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7477b0df4461c18fad05b3a984374ba6\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 2f94da05e9e0c2a58f892c826041add3\n",
      "calculating score for weaving config md5sum: e9327171880c69ee77c0d83f9136ffc6\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 508244f8b9f41113283f4f505470ff74\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 66c987fbf7074f61ba42c6ccbbab6b48\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: c980a58dbdf669696ee346805773897d\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 46.3s, 0.8min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: f77210293c896f1e3f9364a09df40e48\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "_____________________________calculate_score_from_weaving_config - 49.1s, 0.8min\n",
      "_____________________________calculate_score_from_weaving_config - 49.1s, 0.8min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 44395fe7f8268661ae501b0892214204\n",
      "_____________________________calculate_score_from_weaving_config - 49.2s, 0.8min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: c80baf908c5563b937079eb44ff047d7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 07df9df670f1169965a49031d86b8689\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 50.6s, 0.8min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/dc1b2852ae29cb3e88668f7884062bbf\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: ef95d8c989d8233d42781f7c266dd193\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 53.1s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 016afe2b882c3a3bdb1ef46c488d7002\n",
      "_____________________________calculate_score_from_weaving_config - 54.5s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 25ba9b78c6aa13a41c13df891fecab15\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "____________________________calculate_score_from_weaving_config - 112.5s, 1.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 3f2d853e2ec4a6ad8619f525645456d7\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 63.3s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: ce3ce581c45f6b1de864589cc09c6124\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 58.4s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: b776c236fad7e9ce9665a8ab216b9116\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 58.8s, 1.0min\n",
      "_____________________________calculate_score_from_weaving_config - 59.4s, 1.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/8efd3b47fb56675098a0d7b1f6eb6427\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: f065ca6cbe8284277900ba81c20adf7d\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.5s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: b301ef2d2f6c48633c68d2260cbb6308\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: eb2173a4b58ad9c3d456689fd7a7d291\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 58.3s, 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/roberta-base-MNLI\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 9cce4dca644dce3041dc349d8161d7cf\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 57.0s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 1905d365a499d6bfa6c46f97c886022d\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 58.6s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: de56012dd9590139ceaec2e90cc533e7\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 59.2s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: a0f578a893c05421ad1a95c0fd5c939b\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 60.9s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 15e6d8d4aea83b894397fb0d5d81139d\n",
      "_____________________________calculate_score_from_weaving_config - 66.9s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 4b64b3cd2caa61676f4520e5bd511359\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 61.4s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: d1318df9ef8891f62f59c7d4f181e68d\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 62.1s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 189779f7e65f8f7549238f0f92c0daed\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 61.7s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: e1ba95106fbbc532e540b17d51fb9aa6\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 63.2s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 401402415c56338cb6ef8ed341420a3a\n",
      "_____________________________calculate_score_from_weaving_config - 63.3s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 8ec56d0788b03e8c0618816bb51d1b41\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 58.6s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: df66bacd9cc893993e976ddaa497308e\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 59.1s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 37a0d74bd6f615f1576cfac4a87718c7\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 61.3s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 4e65c641d3df1551d758cfbad9d8ad31\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 60.4s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: f4c8754aef64cc9c0fe6d24e5fd4ef03\n",
      "_____________________________calculate_score_from_weaving_config - 60.7s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 82c3f1d90b2f649e205cd2c2f14f795e\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 57.7s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: b93f2784799ece4b39dfc11259c449f2\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 61.3s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 52fe1d66d1e29eada9c9c0aa7c5fd53f\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.7s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 4fbd88ba7530546e915072a2118941a9\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 59.3s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: a7d69b4c69274be175b0128640e73b29\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 60.4s, 1.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/0831de63b7c0900bbc905f64fa541969\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 972dac89684b064bd56d1f39f26e5e36\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 58.5s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: ba8b1493b75eb59b5e8614da0789ae8c\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.5s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 868aec695a17985a09597ac03d213972\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 58.9s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 9516a9bcb0bf5caf4323b5a0c02068cd\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.9s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: cdc664eb72f603ae3e258111f3f567c9\n",
      "_____________________________calculate_score_from_weaving_config - 59.6s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 784a2c734281db074a622ae6c3363102\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 60.5s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 9ec8e17feb4b15144b6d68adde5065a1\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.9s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 686d5d85f350f2030669cde576489a87\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 60.2s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 3ff514483def76462c4fbb35cce2354b\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 60.2s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 647671cf7d5de4d1a0492b808fea65fe\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "_____________________________calculate_score_from_weaving_config - 60.1s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 5c8788e4ad2d91a65a76faf8dabd3250\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 59.0s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 5b3d1da8b404fb0da07d9fa30e991cd8\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 63.6s, 1.1min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 83c0d916678b2801627cabf4a75ebffd\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 60.4s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: fdf17003b7843f2ecd8f6cd98bc47d9a\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 61.6s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 476ddbf5d5cd88eabb7ab49c276a6ee5\n",
      "_____________________________calculate_score_from_weaving_config - 61.5s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 29b97f72e4ff3927a6a7b99a147f2074\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 57.1s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: e37772a83bd30ce75e6fa07d3d579e49\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.7s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: f98da7c8dead921c86dd1d08767bf710\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 56.4s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: d6563536c0d1d13b8f062610257b1b9a\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.9s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: b38627e7afb5b29ed66054dba87a1a9c\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "_____________________________calculate_score_from_weaving_config - 56.3s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 9b377a40e5263b70cce95ee255b088d3\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.0s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 9de1ff23bb7e198763a30364452bdf3b\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 57.1s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 06c01ec0105d32751fd6c6bae9dfb8a9\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 57.9s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: f6ba93335d1206556cbe333f6abdafee\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.2s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: daf7949b538938a94f88a90ca51c3fb6\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 57.8s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: d1e0a6e17a7362454f3c79ed359e7880\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 57.7s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 7cf2ca7a46ad08073153fba3f862d9b2\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 59.5s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 2755ccb94c046f3de27aeb0c065aab07\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "_____________________________calculate_score_from_weaving_config - 56.3s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 36f0eddfa9db497e21e5cd913cd92cfd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 57.7s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 3d506f729a280f007f031a6ef3b3f3ac\n",
      "_____________________________calculate_score_from_weaving_config - 59.1s, 1.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 0e639b39d131fd30bf2b89885413582d\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 56.9s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 507b5472df9f3ffd7abfbd0ca3118903\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 56.2s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 1df1be6fa737d825e786e2d91042c74a\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 56.5s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 7ff22c64044ff24306069aa89467261e\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 56.5s, 0.9min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e6e9002e24b58507b833ff1081f4f7b2\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: d4d2f9a47867860f6a2e42b2b1be417f\n",
      "_____________________________calculate_score_from_weaving_config - 56.6s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 0fd92b2d1a18ff23bcb5e410725da2fa\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 54.5s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: a42d5c8c783603dcda9c22b64451431b\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 54.7s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 760eb85855d148d9b7f7a7713a9b1254\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 54.3s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 2163aa0a3027d7bcce66a36b8c4a4916\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.1s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 2c62d062a8b17b86556786e8bd5b7d1a\n",
      "_____________________________calculate_score_from_weaving_config - 55.9s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 03cfa09bdf8dcf50c5e140d1047ed060\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 55.5s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: a5ffb9b56a9deddbe85bf3cc58979dce\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 53.1s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 7dacea4ecc14ad5af17dc4e281b059c9\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.7s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 3d17be0997d1c2464f59ec7b62ff6c45\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.5s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: bc28dcfd42972a7f57b7495bf0fdb9a6\n",
      "_____________________________calculate_score_from_weaving_config - 55.6s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: fabd8a4597ca61755f66081cfe5c7cae\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 54.1s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 3326bd4fb0376f7944c9a8392d97102f\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 53.4s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 2ee8e22b67219052509f5bd12bb61948\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.3s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: d6c0ca6074c96cb714fed774d0f65ff6\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.0s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 2e51aa5fc9ffd62323ecb97544bb3cac\n",
      "_____________________________calculate_score_from_weaving_config - 55.7s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 170e2ce7b2854c65e1660e39fc0eaa6b\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 55.6s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: d3ab95ed411499015d3d17b101a26130\n",
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________calculate_score_from_weaving_config - 56.5s, 0.9min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling llm_weaver.calculate_score_from_weaving_config...\n",
      "calculate_score_from_weaving_config({ 'blank_model_config': { 'add_cross_attention': False,\n",
      "                          'architectures': ['RobertaForSequenceClassification'],\n",
      "                          'attention_probs_dropout_prob': 0.1,\n",
      "                          'bad_words_ids': None,\n",
      "                          'begin_suppress_tokens': None,\n",
      "                          'bos_token_id': 0,\n",
      "                          'chunk_size_feed_forward': 0,\n",
      "                          'classifier_dropout': None,\n",
      "                          'cross_attention_hidden_size': None,\n",
      "                          'decoder_start_token_id': None,\n",
      "                          'diversity_penalty': 0.0,\n",
      "                          'do_sample': False,\n",
      "                    ..., n_examples=4096, split='validation')\n",
      "calculating score for weaving config md5sum: 294c91ea24b12f6db50467b5257ea45d\n",
      "Loading textattack/roberta-base-MNLI\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hivamoh/Desktop/CS194/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All predictions are the same! Is your model broken? [1]\n",
      "_____________________________calculate_score_from_weaving_config - 56.9s, 0.9min\n",
      "_____________________________calculate_score_from_weaving_config - 53.4s, 0.9min\n",
      "_____________________________calculate_score_from_weaving_config - 53.4s, 0.9min\n",
      "_____________________________calculate_score_from_weaving_config - 47.1s, 0.8min\n",
      "_____________________________calculate_score_from_weaving_config - 34.2s, 0.6min\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracies, weave_configs = sample_config_to_plots(\n",
    "    dict(p=0.5, seed=42, max_configs=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the models that did so well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7256317689530686\n",
      "{'blank_model_config': {'add_cross_attention': False,\n",
      "                        'architectures': ['RobertaForSequenceClassification'],\n",
      "                        'attention_probs_dropout_prob': 0.1,\n",
      "                        'bad_words_ids': None,\n",
      "                        'begin_suppress_tokens': None,\n",
      "                        'bos_token_id': 0,\n",
      "                        'chunk_size_feed_forward': 0,\n",
      "                        'classifier_dropout': None,\n",
      "                        'cross_attention_hidden_size': None,\n",
      "                        'decoder_start_token_id': None,\n",
      "                        'diversity_penalty': 0.0,\n",
      "                        'do_sample': False,\n",
      "                        'early_stopping': False,\n",
      "                        'encoder_no_repeat_ngram_size': 0,\n",
      "                        'eos_token_id': 2,\n",
      "                        'exponential_decay_length_penalty': None,\n",
      "                        'finetuning_task': 'glue:rte',\n",
      "                        'forced_bos_token_id': None,\n",
      "                        'forced_eos_token_id': None,\n",
      "                        'gradient_checkpointing': False,\n",
      "                        'hidden_act': 'gelu',\n",
      "                        'hidden_dropout_prob': 0.1,\n",
      "                        'hidden_size': 768,\n",
      "                        'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      "                        'initializer_range': 0.02,\n",
      "                        'intermediate_size': 3072,\n",
      "                        'is_decoder': False,\n",
      "                        'is_encoder_decoder': False,\n",
      "                        'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      "                        'layer_norm_eps': 1e-05,\n",
      "                        'length_penalty': 1.0,\n",
      "                        'max_length': 20,\n",
      "                        'max_position_embeddings': 514,\n",
      "                        'min_length': 0,\n",
      "                        'model_type': 'roberta',\n",
      "                        'no_repeat_ngram_size': 0,\n",
      "                        'num_attention_heads': 12,\n",
      "                        'num_beam_groups': 1,\n",
      "                        'num_beams': 1,\n",
      "                        'num_hidden_layers': 12,\n",
      "                        'num_return_sequences': 1,\n",
      "                        'output_attentions': False,\n",
      "                        'output_hidden_states': False,\n",
      "                        'output_scores': False,\n",
      "                        'pad_token_id': 1,\n",
      "                        'position_embedding_type': 'absolute',\n",
      "                        'prefix': None,\n",
      "                        'problem_type': None,\n",
      "                        'pruned_heads': {},\n",
      "                        'remove_invalid_values': False,\n",
      "                        'repetition_penalty': 1.0,\n",
      "                        'return_dict': True,\n",
      "                        'return_dict_in_generate': False,\n",
      "                        'sep_token_id': None,\n",
      "                        'suppress_tokens': None,\n",
      "                        'task_specific_params': None,\n",
      "                        'temperature': 1.0,\n",
      "                        'tf_legacy_loss': False,\n",
      "                        'tie_encoder_decoder': False,\n",
      "                        'tie_word_embeddings': True,\n",
      "                        'tokenizer_class': None,\n",
      "                        'top_k': 50,\n",
      "                        'top_p': 1.0,\n",
      "                        'torch_dtype': None,\n",
      "                        'torchscript': False,\n",
      "                        'transformers_version': '4.35.0',\n",
      "                        'type_vocab_size': 1,\n",
      "                        'typical_p': 1.0,\n",
      "                        'use_bfloat16': False,\n",
      "                        'use_cache': True,\n",
      "                        'vocab_size': 50265},\n",
      " 'classification_head': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                         'type': 'SingleClassificationHead'},\n",
      " 'embeddings': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                'type': 'SingleEmbeddings'},\n",
      " 'glue_task': 'rte',\n",
      " 'layer_assignments': [{'params': {'donor': 'textattack/roberta-base-MNLI',\n",
      "                                   'hidden_layer_number': 0},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-MNLI',\n",
      "                                   'hidden_layer_number': 1},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-MNLI',\n",
      "                                   'hidden_layer_number': 2},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 3},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-MNLI',\n",
      "                                   'hidden_layer_number': 4},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 5},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 6},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 7},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 8},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 9},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-MNLI',\n",
      "                                   'hidden_layer_number': 10},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 11},\n",
      "                        'type': 'SingleLayer'}],\n",
      " 'tokenizer_model_id': 'textattack/roberta-base-RTE'}\n"
     ]
    }
   ],
   "source": [
    "# Get max accuracy index\n",
    "max_accuracy_index = accuracies.index(max(accuracies))\n",
    "# accuracies\n",
    "\n",
    "# Get the best config/\n",
    "best_config = weave_configs[max_accuracy_index]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(max(accuracies))\n",
    "pprint(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': {'donor': 'textattack/roberta-base-MNLI', 'hidden_layer_number': 0},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-MNLI', 'hidden_layer_number': 1},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-MNLI', 'hidden_layer_number': 2},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-MNLI', 'hidden_layer_number': 4},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-MNLI',\n",
      "             'hidden_layer_number': 10},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11},\n",
      "  'type': 'SingleLayer'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(best_config['layer_assignments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeating one Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to sample configs\n",
    "import random\n",
    "\n",
    "from llm_weaver import dict_overwrite, get_model_config\n",
    "\n",
    "\n",
    "def sample_weave_configs_iter_layers(p=0.5, seed=42, max_configs=1):\n",
    "\n",
    "    donor_model_ids = [\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "    ]\n",
    "    blank_model_config = dict_overwrite(\n",
    "        get_model_config(\"textattack/roberta-base-RTE\"),\n",
    "        {\n",
    "            \"num_hidden_layers\": 13 ,\n",
    "        },\n",
    "    )\n",
    "    num_hidden_layers = 13\n",
    "    layers_to_repeat = [i for i in range(num_hidden_layers-1)]\n",
    "    print(layers_to_repeat)\n",
    "    \n",
    "    for config_index in range(max_configs):\n",
    "        repeat_layer = layers_to_repeat[config_index]\n",
    "        # repeat_layer = 0\n",
    "        layer_assignments = []\n",
    "        for i in range(num_hidden_layers-1):\n",
    "            layer_assignment = {\n",
    "                \"type\": \"SingleLayer\",\n",
    "                \"params\": {\n",
    "                    \"donor\": random.choices(donor_model_ids, weights=[p, 1 - p])[0],\n",
    "                    \"hidden_layer_number\": i,\n",
    "                },\n",
    "            }\n",
    "            layer_assignments.append(layer_assignment)\n",
    "            \n",
    "            print(layer_assignment)\n",
    "\n",
    "            # Repeat the selected layer\n",
    "            if i == repeat_layer:\n",
    "                # print(i) \n",
    "                layer_assignment = {\n",
    "                    \"type\": \"SingleLayer\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": random.choices(donor_model_ids, weights=[p, 1 - p])[0],\n",
    "                        \"hidden_layer_number\": i,\n",
    "                        },\n",
    "                    }\n",
    "                print(layer_assignment)\n",
    "                layer_assignments.append(layer_assignment)\n",
    "        print(len(layer_assignments))\n",
    "        config = {\n",
    "            \"glue_task\": \"rte\",\n",
    "            \"tokenizer_model_id\": \"textattack/roberta-base-RTE\",\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": layer_assignments,\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        yield config  \n",
    "\n",
    "\n",
    "sample_config = dict(p=0.5, seed=42, max_configs=12)\n",
    "\n",
    "# Generate the sample configs and save to a file just in case\n",
    "weave_configs = list(sample_weave_configs_iter_layers(**sample_config))\n",
    "\n",
    "\n",
    "len(weave_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}\n",
      "13\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/44fdc3884589e08b3574c913901f2cf0\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1f904f164a99bd9f084d231f384f4a09\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/07e16c21875a169ae1e979070e57358a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e7b70d6f126480910383fe3cf9fb5d54\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/33210a36bd65125f103bde0b35c8a3bd\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1431e74bae4e6e98c04c683821bd65f4\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a6921873d2d74fdcbb110231545cb90a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/6332556572d8efa42d027e759656696e\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7172fa7f16e4c9acfb2704ac4b12ffe5\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a73f25dcf682354e62d9b2da45b41611\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/06263e05cb0c531e6c61800376fa1f3d\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e275d373e677681ac011bb6901f714a0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[0.6028880866425993, 0.6101083032490975, 0.6678700361010831, 0.6967509025270758, 0.6895306859205776, 0.6859205776173285, 0.703971119133574, 0.7364620938628159, 0.7148014440433214, 0.7292418772563177, 0.7184115523465704, 0.7256317689530686]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracies = []\n",
    "\n",
    "def sample_config_to_plots(sample_config, n_jobs=5):\n",
    "    weave_configs = list(\n",
    "        sample_weave_configs_iter_layers(**sample_config),\n",
    "    )\n",
    "    # for config in weave_configs:\n",
    "        \n",
    "    #     print(config['layer_assignments'])\n",
    "    scores = Parallel(n_jobs=n_jobs, return_as=\"list\")(\n",
    "        delayed(calculate_score_from_weaving_config_cached)(\n",
    "            weave_config,\n",
    "            n_examples=4096,\n",
    "            split=\"validation\",\n",
    "        )\n",
    "        for weave_config in weave_configs\n",
    "    )\n",
    "    accuracies = [score[\"accuracy\"] for score in scores]\n",
    "    print(accuracies)\n",
    "\n",
    "    title = f\"Accuracy distribution on task {weave_configs[0]['glue_task']} with p={sample_config['p']} with N={len(accuracies)}\"\n",
    "\n",
    "    # create figure and ax\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(accuracies, bins=10)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return accuracies, weave_configs\n",
    "\n",
    "\n",
    "accuracies, weave_configs = sample_config_to_plots(\n",
    "    dict(p=0.5, seed=42, max_configs=12),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7364620938628159\n",
      "{'blank_model_config': {'add_cross_attention': False,\n",
      "                        'architectures': ['RobertaForSequenceClassification'],\n",
      "                        'attention_probs_dropout_prob': 0.1,\n",
      "                        'bad_words_ids': None,\n",
      "                        'begin_suppress_tokens': None,\n",
      "                        'bos_token_id': 0,\n",
      "                        'chunk_size_feed_forward': 0,\n",
      "                        'classifier_dropout': None,\n",
      "                        'cross_attention_hidden_size': None,\n",
      "                        'decoder_start_token_id': None,\n",
      "                        'diversity_penalty': 0.0,\n",
      "                        'do_sample': False,\n",
      "                        'early_stopping': False,\n",
      "                        'encoder_no_repeat_ngram_size': 0,\n",
      "                        'eos_token_id': 2,\n",
      "                        'exponential_decay_length_penalty': None,\n",
      "                        'finetuning_task': 'glue:rte',\n",
      "                        'forced_bos_token_id': None,\n",
      "                        'forced_eos_token_id': None,\n",
      "                        'gradient_checkpointing': False,\n",
      "                        'hidden_act': 'gelu',\n",
      "                        'hidden_dropout_prob': 0.1,\n",
      "                        'hidden_size': 768,\n",
      "                        'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      "                        'initializer_range': 0.02,\n",
      "                        'intermediate_size': 3072,\n",
      "                        'is_decoder': False,\n",
      "                        'is_encoder_decoder': False,\n",
      "                        'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      "                        'layer_norm_eps': 1e-05,\n",
      "                        'length_penalty': 1.0,\n",
      "                        'max_length': 20,\n",
      "                        'max_position_embeddings': 514,\n",
      "                        'min_length': 0,\n",
      "                        'model_type': 'roberta',\n",
      "                        'no_repeat_ngram_size': 0,\n",
      "                        'num_attention_heads': 12,\n",
      "                        'num_beam_groups': 1,\n",
      "                        'num_beams': 1,\n",
      "                        'num_hidden_layers': 13,\n",
      "                        'num_return_sequences': 1,\n",
      "                        'output_attentions': False,\n",
      "                        'output_hidden_states': False,\n",
      "                        'output_scores': False,\n",
      "                        'pad_token_id': 1,\n",
      "                        'position_embedding_type': 'absolute',\n",
      "                        'prefix': None,\n",
      "                        'problem_type': None,\n",
      "                        'pruned_heads': {},\n",
      "                        'remove_invalid_values': False,\n",
      "                        'repetition_penalty': 1.0,\n",
      "                        'return_dict': True,\n",
      "                        'return_dict_in_generate': False,\n",
      "                        'sep_token_id': None,\n",
      "                        'suppress_tokens': None,\n",
      "                        'task_specific_params': None,\n",
      "                        'temperature': 1.0,\n",
      "                        'tf_legacy_loss': False,\n",
      "                        'tie_encoder_decoder': False,\n",
      "                        'tie_word_embeddings': True,\n",
      "                        'tokenizer_class': None,\n",
      "                        'top_k': 50,\n",
      "                        'top_p': 1.0,\n",
      "                        'torch_dtype': None,\n",
      "                        'torchscript': False,\n",
      "                        'transformers_version': '4.35.0',\n",
      "                        'type_vocab_size': 1,\n",
      "                        'typical_p': 1.0,\n",
      "                        'use_bfloat16': False,\n",
      "                        'use_cache': True,\n",
      "                        'vocab_size': 50265},\n",
      " 'classification_head': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                         'type': 'SingleClassificationHead'},\n",
      " 'embeddings': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                'type': 'SingleEmbeddings'},\n",
      " 'glue_task': 'rte',\n",
      " 'layer_assignments': [{'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 0},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 1},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 2},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 3},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 4},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 5},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 6},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 7},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 7},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 8},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 9},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 10},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 11},\n",
      "                        'type': 'SingleLayer'}],\n",
      " 'tokenizer_model_id': 'textattack/roberta-base-RTE'}\n"
     ]
    }
   ],
   "source": [
    "# Get max accuracy index\n",
    "max_accuracy_index = accuracies.index(max(accuracies))\n",
    "# accuracies\n",
    "\n",
    "# Get the best config\n",
    "best_config = weave_configs[max_accuracy_index]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(max(accuracies))\n",
    "pprint(best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11},\n",
      "  'type': 'SingleLayer'}]\n"
     ]
    }
   ],
   "source": [
    "pprint(best_config['layer_assignments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipping a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}]\n"
     ]
    }
   ],
   "source": [
    "# Function to sample configs\n",
    "import random\n",
    "\n",
    "from llm_weaver import dict_overwrite, get_model_config\n",
    "\n",
    "\n",
    "def sample_weave_configs_iter_layers(p=0.5, seed=42, max_configs=1):\n",
    "\n",
    "    donor_model_ids = [\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "        # \"textattack/roberta-base-MNLI\",\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "    ]\n",
    "    blank_model_config = dict_overwrite(\n",
    "        get_model_config(\"textattack/roberta-base-RTE\"),\n",
    "        {\n",
    "            \"num_hidden_layers\": 11,\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    skip_layer = 0  # Initialize skip_layer variable\n",
    "\n",
    "    for _ in range(max_configs):\n",
    "        # print(skip_layer)\n",
    "        config = {\n",
    "            \"glue_task\": \"rte\",\n",
    "            \"tokenizer_model_id\": \"textattack/roberta-base-RTE\",\n",
    "            \"blank_model_config\": blank_model_config,\n",
    "            \"layer_assignments\": [\n",
    "                {\n",
    "                    \"type\": \"SingleLayer\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": random.choices(donor_model_ids, weights=[p, 1 - p])[0],\n",
    "                        \"hidden_layer_number\": i,\n",
    "                    },\n",
    "                }\n",
    "                for i in range(12) if i != skip_layer \n",
    "            ],\n",
    "            \"classification_head\": {\n",
    "                \"type\": \"SingleClassificationHead\",\n",
    "                \"params\": {\n",
    "                    \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                },\n",
    "            },\n",
    "            \"embeddings\": {\n",
    "                \"type\": \"SingleEmbeddings\",\n",
    "                \"params\": {\n",
    "                    \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "        print(config['layer_assignments'])\n",
    "        # print(len(config['layer_assignments']))\n",
    "        yield config\n",
    "        skip_layer = (skip_layer + 1) % 12  # Increment skip_layer for the next configuration\n",
    "\n",
    "\n",
    "\n",
    "sample_config = dict(p=0.5, seed=42, max_configs=12)\n",
    "# Generate the sample configs and save to a file just in case\n",
    "weave_configs = list(sample_weave_configs_iter_layers(**sample_config))\n",
    "\n",
    "\n",
    "# len(weave_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11}}]\n",
      "[{'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9}}, {'type': 'SingleLayer', 'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10}}]\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e048c25430dd88f10ac2ed06303cddb9\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/8f592012e1ab11728a708b7a8ddd286a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c59594d710ecc9c6f01dbc3d12183ba1\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/299099745ba6536703ff5ba957c7d64c\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/5529c59b8a58263ece1d6098401852fa\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7d6277e496caf1f524b26bdd6aae5b4f\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/67ae42f9bee9216a791a7e2d656ef906\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a65acd79618167797f85194440231c19\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/46694124278cfeb93dc8e887615934ea\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/9c7bc0d683fdc1dcf0a7620b2ce0b8c7\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c519fcbcb99cc261c7b3a2e20f2e22db\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/2bcd4e9c6da2f104fae1f5db547b9113\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[0.5090252707581228, 0.5884476534296029, 0.5523465703971119, 0.5379061371841155, 0.5992779783393501, 0.51985559566787, 0.5342960288808665, 0.47653429602888087, 0.555956678700361, 0.6498194945848376, 0.7075812274368231, 0.628158844765343]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sample_config_to_plots(sample_config, n_jobs=5):\n",
    "    accuracies = []\n",
    "    weave_configs = list(\n",
    "        sample_weave_configs_iter_layers(**sample_config),\n",
    "    )\n",
    "    # for config in weave_configs:\n",
    "        \n",
    "    #     # print(config['layer_assignments'])\n",
    "    scores = Parallel(n_jobs=n_jobs, return_as=\"list\")(\n",
    "        delayed(calculate_score_from_weaving_config_cached)(\n",
    "            weave_config,\n",
    "            n_examples=4096,\n",
    "            split=\"validation\",\n",
    "        )\n",
    "        for weave_config in weave_configs\n",
    "    )\n",
    "    accuracies = [score[\"accuracy\"] for score in scores]\n",
    "    print(accuracies)\n",
    "\n",
    "    title = f\"Accuracy distribution on task {weave_configs[0]['glue_task']} with p={sample_config['p']} with N={len(accuracies)}\"\n",
    "\n",
    "    # create figure and ax\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(accuracies, bins=10)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return accuracies, weave_configs\n",
    "\n",
    "\n",
    "accuracies, weave_configs = sample_config_to_plots(\n",
    "    dict(p=0.5, seed=42, max_configs=12),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7075812274368231\n",
      "[{'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11},\n",
      "  'type': 'SingleLayer'}]\n"
     ]
    }
   ],
   "source": [
    "# Get max accuracy index\n",
    "max_accuracy_index = accuracies.index(max(accuracies))\n",
    "# accuracies\n",
    "\n",
    "# Get the best config\n",
    "best_config = weave_configs[max_accuracy_index]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(max(accuracies))\n",
    "# pprint(best_config)\n",
    "pprint(best_config['layer_assignments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heat map - Trying all permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "144\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "from itertools import product\n",
    "from llm_weaver import dict_overwrite, get_model_config\n",
    "\n",
    "def sample_weave_configs_iter_layers(p=0.5, seed=42, max_configs=144):\n",
    "    donor_model_ids = [\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "    ]\n",
    "    base_model_config = get_model_config(\"textattack/roberta-base-RTE\")\n",
    "    config_id = 0  # Initialize config_id to keep track of the number of configurations\n",
    "    random.seed(seed)\n",
    "    # all_pairs = list(product(range(12), repeat=2))  # Create pairs (x, y) for x, y in range(12)\n",
    "\n",
    "    # for x, y in all_pairs[:max_configs]:  # Limit to max_configs\n",
    "    for x in range(12):\n",
    "        for y in range(12):\n",
    "            if config_id >= max_configs:\n",
    "                return  # Stop when the maximum number of configurations is reached\n",
    "            layer_assignments = [\n",
    "                {\n",
    "                    \"type\": \"SingleLayer\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": random.choices(donor_model_ids, weights=[p, 1 - p])[0],\n",
    "                        \"hidden_layer_number\": i,\n",
    "                    },\n",
    "                }\n",
    "                for i in list(range(0, x + 1)) + list(range(y, 12))  # Range from 0 to x+1 and y to num_hidden_layers\n",
    "            ]\n",
    "\n",
    "            # Update the num_hidden_layers in the blank_model_config\n",
    "            blank_model_config = dict_overwrite(\n",
    "                base_model_config,\n",
    "                {\n",
    "                    \"num_hidden_layers\": len(layer_assignments),\n",
    "                },\n",
    "            )\n",
    "            print(len(layer_assignments))\n",
    "            config = {\n",
    "                \"glue_task\": \"rte\",\n",
    "                \"tokenizer_model_id\": \"textattack/roberta-base-RTE\",\n",
    "                \"blank_model_config\": blank_model_config,\n",
    "                \"layer_assignments\": layer_assignments,\n",
    "                \"classification_head\": {\n",
    "                    \"type\": \"SingleClassificationHead\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                    },\n",
    "                },\n",
    "                \"embeddings\": {\n",
    "                    \"type\": \"SingleEmbeddings\",\n",
    "                    \"params\": {\n",
    "                        \"donor\": \"textattack/roberta-base-RTE\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "            yield config\n",
    "\n",
    "sample_config = dict(p=0.5, seed=42, max_configs=144)\n",
    "# Generate the sample configs and save to a file just in case\n",
    "weave_configs = list(sample_weave_configs_iter_layers(**sample_config))\n",
    "print(len(weave_configs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "8\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "9\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "24\n",
      "23\n",
      "22\n",
      "21\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/8f592012e1ab11728a708b7a8ddd286a\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/44fdc3884589e08b3574c913901f2cf0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/493c7eba36d1b8379b200cf9d041e2f7\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/eddd469053910619c4a4aef48c4d5836\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/56a3d01021231def01ad57ba2395526b\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fb213b7213d3bed631c1fd7785c63b3b\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/6ef620dc006b9352eb7cae8a74ec70d1\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/f87e2109bc3bc704951dbba39c422fbb\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/ea1cb8c6b53f8a995c4bcb30006fe05b\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fd93a972f1ef04dce87753046936eba1\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/34f4ee006135eee6a2567c2019f1509a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c2a2495693d318deaf072aa613b43b22\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1f904f164a99bd9f084d231f384f4a09\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/299099745ba6536703ff5ba957c7d64c\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/85966a45ff568d2876b02eba34e3427b\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/062b9387062468a4bcd452407a629a93\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/521cdf775426b616b129d8482f2b22b5\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a590bf5c84476bd5c8741ddb18e6976a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/47be0c0fb1374f90e3c35fb91ff96c6c\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/cb3f841b6d2a40c2d3dd1d868f842087\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/41b1b22aae1f97161b05e23a8916af59\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/df7a0a9be8377af5c69be8f7635700ba\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/76244a280a026ebe4682039667a098a7\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/4e829faebb6e39f8400afac66dc351a1\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/07e16c21875a169ae1e979070e57358a\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c59594d710ecc9c6f01dbc3d12183ba1\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/102626220724af3e1c1da6f680d53ab4\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/5dac8515284725c20e3360550b2f8844\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fedbd1e75dafe8ad628d2694c1cf5338\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/9bd4d676c1363b1a9b66c398e416d4f0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/06bfa7d63e901b709594d6a99a2e2f32\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/724468e61624ea4110c0e49d898373ea\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/393600b07f2fb96232fc3a90437b2929\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e7b70d6f126480910383fe3cf9fb5d54\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/28e36bc3bbcfdff50ca358cd893d1dac\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/b6abe20786a5ed7cd2a9f76a8583ee12\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/5529c59b8a58263ece1d6098401852fa\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/761f5bae665cdd72c7521f2df3e888ab\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d2cd195c780afacd854fd75b0ad31c37\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/9812e7fddf388eddf89c0d2a6decd4c8\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/19d8afb1f26abf884dc4d04ad8cd9e02\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d11a4800316c033d7a5a126c59f25abd\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d8d834dbc57d5c8c971af05bea22e997\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d966812794cb48c6b905c3ab14c880da\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d86b4ec0097d5a5c253eb40d4c32a2dc\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/98cfd5f7452af5c4670629232c559af0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/da61dbc3303e82718920c789dcfb2170\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/b3b999a6f323aa695c4aa348771b72d0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7dfe51a3f2ec98b4fb6ba0a0efde0ba8\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/67ae42f9bee9216a791a7e2d656ef906[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/33210a36bd65125f103bde0b35c8a3bd\n",
      "\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/b4184c808e032df119d75a38d2e81191\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7329b0dbe55d0f63cf451e559727bbc6\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/70f50532bd878e5e621cf19862983b06\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7d6277e496caf1f524b26bdd6aae5b4f\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/3a823177835373eaa215b034c87fe39e\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/50ef9ae42366c15d4964bc4c1af6e87c\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/ac7ba3698853aa1759d76b55299400a2\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/75a69c3eed9d8da681b2892f06274303\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fbd939069745d454c5a597db832d2e3b\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/8689e5f7051dc5b57ac053ab04be7b46\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/b6a5a9754a0155cbeb7877bb135e1046\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/949405fbb58abca621d00706a59b5fe9\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1431e74bae4e6e98c04c683821bd65f4\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d6e4b732954f1f129b6433d8486a27e8\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/68e8470fc2f3f04aa984994617b303fa\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/f2a6b123861b4d8b4ab6cb99e63fb3ba\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e061f83cbb596549762269a86d85572f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e9cad7be69d2c72c583dae9002f4988e\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/6a35aa70e933d32ae9f8dc8240057790\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/995c20da7cbb3a89d0f6ea6b2d76a14d\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a65acd79618167797f85194440231c19\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d7824771dc233701107cb66e854a553b\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/aad92885c6ad37b7e2eb3d05281d1f7c\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d4df5cc44a3402e94dfd390dd65353c0\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/8c3ab95d02235b980860aac1ded00a48\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/dafe5feadf42532632b14f4a23e133c2\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a6921873d2d74fdcbb110231545cb90a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/0cdeaec68f0a5c8cc0e1f4edd0d2d4ba\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/6332556572d8efa42d027e759656696e\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a3dc075d2844f1e4ab26cf8bf28458a4\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/1e8a2caf6c358b15dd7bbe1e289f2d22\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d19e0e0416ec09241138a151dbe1b519[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fabb9340d9d0fdbf6c565241973139df\n",
      "\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d76776a6bf2e1ef075084861b7ae188a\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c9a21207b18fb51a894d4b0788178367\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/bd390df6b39edf5d8ab369335c40008b\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fca4c24eb0aadf83ab52f03d5fd6cc03\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/46694124278cfeb93dc8e887615934ea\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e1a6edebb7736ea3d06fde14d9f93960\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7172fa7f16e4c9acfb2704ac4b12ffe5\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/f2391ed774d03c52d13add562bd1a9b6\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c519fcbcb99cc261c7b3a2e20f2e22db\n",
      "\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a6a3892e74f39eeed8e94d294c9e89e4\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/3fe4472e45115cba28e0013bd200c3cc\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/29bc44895214c29e5b2942cb747b788f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c31d4d1daf27ae13918c1da2f28eaf37\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/42d5e3cf43c1c614ee84188bca0758a6\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/9c7bc0d683fdc1dcf0a7620b2ce0b8c7\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/61f910c32145ab6ab8a972255592f38e\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a762e55ccdbb050a0da7db33a796d923\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/29d674162db50e6d510f5e094ec16147\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/ebe4894a57c992376286454d101b0d81\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/0a2193f670254e62f4603460f6f6ec71\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7f3155e7ea1e4f6b0f23c46068330a1e\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/d0eb6f379e726cc2616e3f3e76de64db\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/28dff8bdd9b5912c4facbe5eb2c7982c\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/692fa6ba1efe241a507c75b7ffdf2b22\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/95b5240fc847468b44ea987f502c89bd\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fe6e102243b27decfdf759aa72882ba3\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/6d5619714242efd0add0f5a1d7086ff3\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/06263e05cb0c531e6c61800376fa1f3d\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/7f1d51dda6dee97a65e5159216c012a1\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c1511796e65b9548b308ecc7cc3f7790\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/cab2e3181b8588617bc8c4e83f876776\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/fec801db7a3ece365918a6ee5acab3a2\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/a73f25dcf682354e62d9b2da45b41611\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/888f708c190d0ff0f28710e433358ae0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/0a3c6c7a1aa2af87ddf8da51d4d52636\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e2a6a6b12485b50805679455a02d975f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/b753079563efd02c6949caf52e833222\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/cd9f5778b67856ebde4b3d6ec902a6b8\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/b42a5b63f5a2cdf280fff224c0ec5063\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c0b11135a6530a10d060c04bd71eacda\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/113b6e3fede670fa18d90dfc096e44d4\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/842cc669f8723cb3b2990366e5ccd54f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/c654f455599c169848a7170f23503415\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/46c9eba7d69d63bcf47421cc2368935d\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e6c189a10529a188370ec0b16e464144\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/f1ec90d1a7b0eb12dce42d212e54f48d\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/64dc90d16f5d8cd0f7b0e656f93e7f65\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/aa338a5d1bd7dfb27a143803821dba1f\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/3860c94ab96b8f455ba8afad124fdd39\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[Memory]: Loading calculate_score_from_weaving_config from cache/joblib/llm_weaver/calculate_score_from_weaving_config/e275d373e677681ac011bb6901f714a0\n",
      "_________________calculate_score_from_weaving_config cache loaded - 0.0s, 0.0min\n",
      "[0.6028880866425993, 0.7256317689530686, 0.5884476534296029, 0.516245487364621, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.5523465703971119, 0.6101083032490975, 0.7256317689530686, 0.5523465703971119, 0.49097472924187724, 0.4404332129963899, 0.4729241877256318, 0.49458483754512633, 0.4693140794223827, 0.48014440433212996, 0.4729241877256318, 0.4729241877256318, 0.516245487364621, 0.5740072202166066, 0.6678700361010831, 0.7256317689530686, 0.5379061371841155, 0.47653429602888087, 0.4729241877256318, 0.48375451263537905, 0.48736462093862815, 0.5090252707581228, 0.4729241877256318, 0.4729241877256318, 0.5090252707581228, 0.555956678700361, 0.6137184115523465, 0.6967509025270758, 0.7256317689530686, 0.5992779783393501, 0.47653429602888087, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.49097472924187724, 0.5342960288808665, 0.5487364620938628, 0.6425992779783394, 0.6895306859205776, 0.7256317689530686, 0.51985559566787, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.48014440433212996, 0.48736462093862815, 0.5415162454873647, 0.48375451263537905, 0.628158844765343, 0.6859205776173285, 0.7256317689530686, 0.5342960288808665, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4584837545126354, 0.48014440433212996, 0.5234657039711191, 0.4729241877256318, 0.5342960288808665, 0.5306859205776173, 0.703971119133574, 0.7256317689530686, 0.47653429602888087, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.49097472924187724, 0.4729241877256318, 0.4729241877256318, 0.48375451263537905, 0.4729241877256318, 0.48736462093862815, 0.7148014440433214, 0.7364620938628159, 0.7256317689530686, 0.555956678700361, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.4729241877256318, 0.48014440433212996, 0.4729241877256318, 0.48014440433212996, 0.5234657039711191, 0.7364620938628159, 0.7148014440433214, 0.7256317689530686, 0.6498194945848376, 0.5884476534296029, 0.48014440433212996, 0.4729241877256318, 0.4729241877256318, 0.48375451263537905, 0.4729241877256318, 0.4981949458483754, 0.5703971119133574, 0.7148014440433214, 0.7184115523465704, 0.7292418772563177, 0.7256317689530686, 0.7075812274368231, 0.48375451263537905, 0.4729241877256318, 0.47653429602888087, 0.49458483754512633, 0.47653429602888087, 0.5270758122743683, 0.5703971119133574, 0.6389891696750902, 0.7075812274368231, 0.7148014440433214, 0.7184115523465704, 0.7256317689530686, 0.4981949458483754, 0.5018050541516246, 0.5451263537906137, 0.555956678700361, 0.5703971119133574, 0.6028880866425993, 0.703971119133574, 0.6678700361010831, 0.703971119133574, 0.7184115523465704, 0.7148014440433214, 0.7256317689530686]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sample_config_to_plots(sample_config, n_jobs=5):\n",
    "    accuracies = []\n",
    "    weave_configs = list(\n",
    "        sample_weave_configs_iter_layers(**sample_config),\n",
    "    )\n",
    "    # for config in weave_configs:\n",
    "        \n",
    "    #     # print(config['layer_assignments'])\n",
    "    scores = Parallel(n_jobs=n_jobs, return_as=\"list\")(\n",
    "        delayed(calculate_score_from_weaving_config_cached)(\n",
    "            weave_config,\n",
    "            n_examples=4096,\n",
    "            split=\"validation\",\n",
    "        )\n",
    "        for weave_config in weave_configs\n",
    "    )\n",
    "    accuracies = [score[\"accuracy\"] for score in scores]\n",
    "    print(accuracies)\n",
    "\n",
    "    title = f\"Accuracy distribution on task {weave_configs[0]['glue_task']} with p={sample_config['p']} with N={len(accuracies)}\"\n",
    "\n",
    "    # create figure and ax\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.hist(accuracies, bins=10)\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "\n",
    "    return accuracies, weave_configs\n",
    "\n",
    "\n",
    "accuracies, weave_configs = sample_config_to_plots(\n",
    "    dict(p=0.5, seed=42, max_configs=12),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7364620938628159\n",
      "[{'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 0},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 1},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 2},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 3},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 4},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 5},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 6},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 7},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 8},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 9},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 10},\n",
      "  'type': 'SingleLayer'},\n",
      " {'params': {'donor': 'textattack/roberta-base-RTE', 'hidden_layer_number': 11},\n",
      "  'type': 'SingleLayer'}]\n"
     ]
    }
   ],
   "source": [
    "# Get max accuracy index\n",
    "max_accuracy_index = accuracies.index(max(accuracies))\n",
    "# accuracies\n",
    "\n",
    "# Get the best config\n",
    "best_config = weave_configs[max_accuracy_index]\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(max(accuracies))\n",
    "# pprint(best_config)\n",
    "pprint(best_config['layer_assignments'])\\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "418d7ffbf17f07fb7db812548652bd547b7709e294d6b5c17ab307303352fd7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
