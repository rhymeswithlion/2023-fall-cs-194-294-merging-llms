{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the weaving code on the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /home/brian/2023-fall-cs-194-294-merging-llms/.venv/bin/pip: bad interpreter: .venv/bin/python3.8: no such file or directory\n",
      "zsh:1: /home/brian/2023-fall-cs-194-294-merging-llms/.venv/bin/pip: bad interpreter: .venv/bin/python3.8: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "\n",
    "! pip install -q joblib  # joblib for memoizing functions\n",
    "! pip install -q ipywidgets widgetsnbextension pandas-profiling # IProgress for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model_merging to the python path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "model_merging_base = os.path.abspath(\"../model_merging/\")\n",
    "# assert it exist\n",
    "assert os.path.exists(model_merging_base)\n",
    "if model_merging_base not in sys.path:\n",
    "    sys.path.append(model_merging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib for caching and distributed computing\n",
    "from math import sqrt\n",
    "\n",
    "from joblib import Memory, Parallel, delayed\n",
    "\n",
    "# memory = Memory(location=\"cache\", verbose=10)\n",
    "memory = Memory(location=\"cache\", verbose=0)\n",
    "\n",
    "parallel = Parallel(n_jobs=2, return_as=\"generator\")\n",
    "output_generator = parallel(delayed(sqrt)(i**2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 15:49:05.842750: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 15:49:05.876491: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 15:49:05.877068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 15:49:06.420500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports and cached functions\n",
    "\n",
    "import os\n",
    "\n",
    "from llm_weaver import (\n",
    "    calculate_score_from_weaving_config,\n",
    "    get_score_from_named_model,\n",
    "    test_weaver,\n",
    ")\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "calculate_score_from_weaving_config_cached = memory.cache(\n",
    "    calculate_score_from_weaving_config\n",
    ")\n",
    "test_weaver_cached = memory.cache(test_weaver)\n",
    "\n",
    "get_score_from_named_model_cached = memory.cache(get_score_from_named_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure you can build using `.build()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have transformers version 4.35.0!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 15:49:09.166790: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-27 15:49:09.167794: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from llm_weaver import get_blank_model, get_model_config\n",
    "\n",
    "if transformers.__version__ < \"4.3.1\":\n",
    "    raise ValueError(\n",
    "        \"Need transformers >= 4.3.1, or something like that. Not sure of the version.\"\n",
    "    )\n",
    "    # https://github.com/huggingface/transformers/commit/4a55e4787760fdb6c40a972a60d814ba05425da1#diff-648ec06beb5ae6380c7f611a0f513a5d392509497d245a09f06b6549358afdffR1151\n",
    "\n",
    "print(f\"You have transformers version {transformers.__version__}!\")\n",
    "\n",
    "model = get_blank_model(get_model_config(\"textattack/roberta-base-RTE\"))\n",
    "model.build()\n",
    "\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Test weaving code\n",
    "\n",
    "This test makes sure that our score when using the weaver to reconstruct a model from all its parts get the same evaluation score as the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 756986b041cd5656d4878287e6fef62f\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/model_merging/model_merging/evaluation.py:7: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  return hfds.load_metric(\"glue\", task)\n",
      "2023-11-27 15:49:20.735385: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 3f302d2841d5e9508a8e33464b313985\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:49:35.352701: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 64b8d72806f9123d7a952fb0df71b630\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:49:47.277399: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:49:57.247862: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Linear combo weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Fisher weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 367fed3b6bf1fd31867ef1207a026479\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:50:13.897509: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: c87dd0438cfc04d8acd9371cc8fb05f9\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:50:28.531990: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 2de68b10f030533e464532058b3b6081\n",
      "Loading textattack/roberta-base-MNLI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:50:41.060051: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:221: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:50:53.688011: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "Weaved score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "Linear combo weaved score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n",
      "Fisher weaved score (textattack/roberta-base-MNLI): {'accuracy': 0.3}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>([{'accuracy': 0.7}, {'accuracy': 0.7}, {'accu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>([{'accuracy': 0.3}, {'accuracy': 0.3}, {'accu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model_id  \\\n",
       "0   textattack/roberta-base-RTE   \n",
       "1  textattack/roberta-base-MNLI   \n",
       "\n",
       "                                             results  \n",
       "0  ([{'accuracy': 0.7}, {'accuracy': 0.7}, {'accu...  \n",
       "1  ([{'accuracy': 0.3}, {'accuracy': 0.3}, {'accu...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_ids = [\n",
    "    \"textattack/roberta-base-RTE\",\n",
    "    \"textattack/roberta-base-MNLI\",  # <--- this one has a very low score\n",
    "    # \"howey/roberta-large-rte\",\n",
    "    # \"howey/roberta-large-mnli\",\n",
    "    # \"howey/roberta-large-qnli\",\n",
    "    # \"howey/roberta-large-sst2\",\n",
    "    # \"howey/roberta-large-cola\",\n",
    "    # \"howey/roberta-large-mrpc\",\n",
    "    # \"howey/roberta-large-qqp\",\n",
    "    # \"howey/roberta-large-stsb\", # <--- did not work\n",
    "    # \"JeremiahZ/roberta-base-rte\",\n",
    "    # \"JeremiahZ/roberta-base-mnli\",\n",
    "    # \"JeremiahZ/roberta-base-qnli\",\n",
    "    # \"JeremiahZ/roberta-base-sst2\",\n",
    "    # \"JeremiahZ/roberta-base-cola\",\n",
    "    # \"JeremiahZ/roberta-base-mrpc\",\n",
    "    # \"JeremiahZ/roberta-base-qqp\",\n",
    "    # \"JeremiahZ/roberta-base-stsb\", # <--- did not work\n",
    "    # \"l-yohai/bigbird-roberta-base-mnli\",\n",
    "    # \"howey/roberta-large-squad2\",\n",
    "]\n",
    "# textattack/roberta-base-RTE ({'accuracy': 0.7}, {'accuracy': 0.7})\n",
    "# textattack/roberta-base-MNLI ({'accuracy': 0.3}, {'accuracy': 0.3})\n",
    "# howey/roberta-large-rte ({'accuracy': 0.65}, {'accuracy': 0.65})\n",
    "# howey/roberta-large-mnli ({'accuracy': 0.68}, {'accuracy': 0.68})\n",
    "# howey/roberta-large-qnli ({'accuracy': 0.86}, {'accuracy': 0.86})\n",
    "# howey/roberta-large-sst2 ({'accuracy': 0.77}, {'accuracy': 0.77})\n",
    "# howey/roberta-large-cola ({'matthews_correlation': 0.19169538058831714}, {'matthews_correlation': 0.19169538058831714})\n",
    "# howey/roberta-large-mrpc ({'accuracy': 0.61, 'f1': 0.6486486486486487}, {'accuracy': 0.61, 'f1': 0.6486486486486487})\n",
    "# howey/roberta-large-qqp ({'accuracy': 0.77, 'f1': 0.5490196078431372}, {'accuracy': 0.77, 'f1': 0.5490196078431372})\n",
    "# JeremiahZ/roberta-base-rte ({'accuracy': 0.61}, {'accuracy': 0.61})\n",
    "# JeremiahZ/roberta-base-mnli ({'accuracy': 0.83}, {'accuracy': 0.83})\n",
    "# JeremiahZ/roberta-base-qnli ({'accuracy': 0.82}, {'accuracy': 0.82})\n",
    "# JeremiahZ/roberta-base-sst2 ({'accuracy': 0.89}, {'accuracy': 0.89})\n",
    "# JeremiahZ/roberta-base-cola ({'matthews_correlation': 0.15285569591066622}, {'matthews_correlation': 0.15285569591066622})\n",
    "# JeremiahZ/roberta-base-mrpc ({'accuracy': 0.33, 'f1': 0.10666666666666666}, {'accuracy': 0.33, 'f1': 0.10666666666666666})\n",
    "# JeremiahZ/roberta-base-qqp ({'accuracy': 0.79, 'f1': 0.7341772151898734}, {'accuracy': 0.79, 'f1': 0.7341772151898734})\n",
    "\n",
    "\n",
    "records = []\n",
    "for model_id in model_ids:\n",
    "    records.append(dict(model_id=model_id, results=str(test_weaver(model_id))))\n",
    "\n",
    "weaving_test_results_df = pd.DataFrame(records)\n",
    "# weaving_test_results_df.to_csv(\"test-weaving-on-base-models.weaving_test_results.csv\")\n",
    "weaving_test_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 756986b041cd5656d4878287e6fef62f\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:51:09.908443: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 3f302d2841d5e9508a8e33464b313985\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:51:24.424171: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating score for weaving config md5sum: 64b8d72806f9123d7a952fb0df71b630\n",
      "Loading textattack/roberta-base-RTE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:51:36.953055: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "/home/brian/2023-fall-cs-194-294-merging-llms/.venv/lib/python3.8/site-packages/transformers/data/processors/glue.py:520: FutureWarning: This processor will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"processor\"), FutureWarning)\n",
      "2023-11-27 15:51:47.421125: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Linear combo weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n",
      "Fisher weaved score (textattack/roberta-base-RTE): {'accuracy': 0.7}\n"
     ]
    }
   ],
   "source": [
    "scores, configs = test_weaver_cached(\"textattack/roberta-base-RTE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "==========\n",
      "==========\n",
      "None\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "{'blank_model_config': {'add_cross_attention': False,\n",
      "                        'architectures': ['RobertaForSequenceClassification'],\n",
      "                        'attention_probs_dropout_prob': 0.1,\n",
      "                        'bad_words_ids': None,\n",
      "                        'begin_suppress_tokens': None,\n",
      "                        'bos_token_id': 0,\n",
      "                        'chunk_size_feed_forward': 0,\n",
      "                        'classifier_dropout': None,\n",
      "                        'cross_attention_hidden_size': None,\n",
      "                        'decoder_start_token_id': None,\n",
      "                        'diversity_penalty': 0.0,\n",
      "                        'do_sample': False,\n",
      "                        'early_stopping': False,\n",
      "                        'encoder_no_repeat_ngram_size': 0,\n",
      "                        'eos_token_id': 2,\n",
      "                        'exponential_decay_length_penalty': None,\n",
      "                        'finetuning_task': 'glue:rte',\n",
      "                        'forced_bos_token_id': None,\n",
      "                        'forced_eos_token_id': None,\n",
      "                        'gradient_checkpointing': False,\n",
      "                        'hidden_act': 'gelu',\n",
      "                        'hidden_dropout_prob': 0.1,\n",
      "                        'hidden_size': 768,\n",
      "                        'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      "                        'initializer_range': 0.02,\n",
      "                        'intermediate_size': 3072,\n",
      "                        'is_decoder': False,\n",
      "                        'is_encoder_decoder': False,\n",
      "                        'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      "                        'layer_norm_eps': 1e-05,\n",
      "                        'length_penalty': 1.0,\n",
      "                        'max_length': 20,\n",
      "                        'max_position_embeddings': 514,\n",
      "                        'min_length': 0,\n",
      "                        'model_type': 'roberta',\n",
      "                        'no_repeat_ngram_size': 0,\n",
      "                        'num_attention_heads': 12,\n",
      "                        'num_beam_groups': 1,\n",
      "                        'num_beams': 1,\n",
      "                        'num_hidden_layers': 12,\n",
      "                        'num_return_sequences': 1,\n",
      "                        'output_attentions': False,\n",
      "                        'output_hidden_states': False,\n",
      "                        'output_scores': False,\n",
      "                        'pad_token_id': 1,\n",
      "                        'position_embedding_type': 'absolute',\n",
      "                        'prefix': None,\n",
      "                        'problem_type': None,\n",
      "                        'pruned_heads': {},\n",
      "                        'remove_invalid_values': False,\n",
      "                        'repetition_penalty': 1.0,\n",
      "                        'return_dict': True,\n",
      "                        'return_dict_in_generate': False,\n",
      "                        'sep_token_id': None,\n",
      "                        'suppress_tokens': None,\n",
      "                        'task_specific_params': None,\n",
      "                        'temperature': 1.0,\n",
      "                        'tf_legacy_loss': False,\n",
      "                        'tie_encoder_decoder': False,\n",
      "                        'tie_word_embeddings': True,\n",
      "                        'tokenizer_class': None,\n",
      "                        'top_k': 50,\n",
      "                        'top_p': 1.0,\n",
      "                        'torch_dtype': None,\n",
      "                        'torchscript': False,\n",
      "                        'transformers_version': '4.35.0',\n",
      "                        'type_vocab_size': 1,\n",
      "                        'typical_p': 1.0,\n",
      "                        'use_bfloat16': False,\n",
      "                        'use_cache': True,\n",
      "                        'vocab_size': 50265},\n",
      " 'classification_head': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                         'type': 'SingleClassificationHead'},\n",
      " 'embeddings': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                'type': 'SingleEmbeddings'},\n",
      " 'glue_task': 'rte',\n",
      " 'layer_assignments': [{'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 0},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 1},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 2},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 3},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 4},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 5},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 6},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 7},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 8},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 9},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 10},\n",
      "                        'type': 'SingleLayer'},\n",
      "                       {'params': {'donor': 'textattack/roberta-base-RTE',\n",
      "                                   'hidden_layer_number': 11},\n",
      "                        'type': 'SingleLayer'}],\n",
      " 'tokenizer_model_id': 'textattack/roberta-base-RTE'}\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "{'blank_model_config': {'add_cross_attention': False,\n",
      "                        'architectures': ['RobertaForSequenceClassification'],\n",
      "                        'attention_probs_dropout_prob': 0.1,\n",
      "                        'bad_words_ids': None,\n",
      "                        'begin_suppress_tokens': None,\n",
      "                        'bos_token_id': 0,\n",
      "                        'chunk_size_feed_forward': 0,\n",
      "                        'classifier_dropout': None,\n",
      "                        'cross_attention_hidden_size': None,\n",
      "                        'decoder_start_token_id': None,\n",
      "                        'diversity_penalty': 0.0,\n",
      "                        'do_sample': False,\n",
      "                        'early_stopping': False,\n",
      "                        'encoder_no_repeat_ngram_size': 0,\n",
      "                        'eos_token_id': 2,\n",
      "                        'exponential_decay_length_penalty': None,\n",
      "                        'finetuning_task': 'glue:rte',\n",
      "                        'forced_bos_token_id': None,\n",
      "                        'forced_eos_token_id': None,\n",
      "                        'gradient_checkpointing': False,\n",
      "                        'hidden_act': 'gelu',\n",
      "                        'hidden_dropout_prob': 0.1,\n",
      "                        'hidden_size': 768,\n",
      "                        'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      "                        'initializer_range': 0.02,\n",
      "                        'intermediate_size': 3072,\n",
      "                        'is_decoder': False,\n",
      "                        'is_encoder_decoder': False,\n",
      "                        'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      "                        'layer_norm_eps': 1e-05,\n",
      "                        'length_penalty': 1.0,\n",
      "                        'max_length': 20,\n",
      "                        'max_position_embeddings': 514,\n",
      "                        'min_length': 0,\n",
      "                        'model_type': 'roberta',\n",
      "                        'no_repeat_ngram_size': 0,\n",
      "                        'num_attention_heads': 12,\n",
      "                        'num_beam_groups': 1,\n",
      "                        'num_beams': 1,\n",
      "                        'num_hidden_layers': 12,\n",
      "                        'num_return_sequences': 1,\n",
      "                        'output_attentions': False,\n",
      "                        'output_hidden_states': False,\n",
      "                        'output_scores': False,\n",
      "                        'pad_token_id': 1,\n",
      "                        'position_embedding_type': 'absolute',\n",
      "                        'prefix': None,\n",
      "                        'problem_type': None,\n",
      "                        'pruned_heads': {},\n",
      "                        'remove_invalid_values': False,\n",
      "                        'repetition_penalty': 1.0,\n",
      "                        'return_dict': True,\n",
      "                        'return_dict_in_generate': False,\n",
      "                        'sep_token_id': None,\n",
      "                        'suppress_tokens': None,\n",
      "                        'task_specific_params': None,\n",
      "                        'temperature': 1.0,\n",
      "                        'tf_legacy_loss': False,\n",
      "                        'tie_encoder_decoder': False,\n",
      "                        'tie_word_embeddings': True,\n",
      "                        'tokenizer_class': None,\n",
      "                        'top_k': 50,\n",
      "                        'top_p': 1.0,\n",
      "                        'torch_dtype': None,\n",
      "                        'torchscript': False,\n",
      "                        'transformers_version': '4.35.0',\n",
      "                        'type_vocab_size': 1,\n",
      "                        'typical_p': 1.0,\n",
      "                        'use_bfloat16': False,\n",
      "                        'use_cache': True,\n",
      "                        'vocab_size': 50265},\n",
      " 'classification_head': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                         'type': 'SingleClassificationHead'},\n",
      " 'embeddings': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                'type': 'SingleEmbeddings'},\n",
      " 'glue_task': 'rte',\n",
      " 'layer_assignments': [{'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': 0.1},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': 0.2},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': 0.30000000000000004},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': 0.4}]},\n",
      "                        'type': 'IsotropicLinearCombination'}],\n",
      " 'tokenizer_model_id': 'textattack/roberta-base-RTE'}\n",
      "==========\n",
      "==========\n",
      "==========\n",
      "{'blank_model_config': {'add_cross_attention': False,\n",
      "                        'architectures': ['RobertaForSequenceClassification'],\n",
      "                        'attention_probs_dropout_prob': 0.1,\n",
      "                        'bad_words_ids': None,\n",
      "                        'begin_suppress_tokens': None,\n",
      "                        'bos_token_id': 0,\n",
      "                        'chunk_size_feed_forward': 0,\n",
      "                        'classifier_dropout': None,\n",
      "                        'cross_attention_hidden_size': None,\n",
      "                        'decoder_start_token_id': None,\n",
      "                        'diversity_penalty': 0.0,\n",
      "                        'do_sample': False,\n",
      "                        'early_stopping': False,\n",
      "                        'encoder_no_repeat_ngram_size': 0,\n",
      "                        'eos_token_id': 2,\n",
      "                        'exponential_decay_length_penalty': None,\n",
      "                        'finetuning_task': 'glue:rte',\n",
      "                        'forced_bos_token_id': None,\n",
      "                        'forced_eos_token_id': None,\n",
      "                        'gradient_checkpointing': False,\n",
      "                        'hidden_act': 'gelu',\n",
      "                        'hidden_dropout_prob': 0.1,\n",
      "                        'hidden_size': 768,\n",
      "                        'id2label': {0: 'LABEL_0', 1: 'LABEL_1'},\n",
      "                        'initializer_range': 0.02,\n",
      "                        'intermediate_size': 3072,\n",
      "                        'is_decoder': False,\n",
      "                        'is_encoder_decoder': False,\n",
      "                        'label2id': {'LABEL_0': 0, 'LABEL_1': 1},\n",
      "                        'layer_norm_eps': 1e-05,\n",
      "                        'length_penalty': 1.0,\n",
      "                        'max_length': 20,\n",
      "                        'max_position_embeddings': 514,\n",
      "                        'min_length': 0,\n",
      "                        'model_type': 'roberta',\n",
      "                        'no_repeat_ngram_size': 0,\n",
      "                        'num_attention_heads': 12,\n",
      "                        'num_beam_groups': 1,\n",
      "                        'num_beams': 1,\n",
      "                        'num_hidden_layers': 12,\n",
      "                        'num_return_sequences': 1,\n",
      "                        'output_attentions': False,\n",
      "                        'output_hidden_states': False,\n",
      "                        'output_scores': False,\n",
      "                        'pad_token_id': 1,\n",
      "                        'position_embedding_type': 'absolute',\n",
      "                        'prefix': None,\n",
      "                        'problem_type': None,\n",
      "                        'pruned_heads': {},\n",
      "                        'remove_invalid_values': False,\n",
      "                        'repetition_penalty': 1.0,\n",
      "                        'return_dict': True,\n",
      "                        'return_dict_in_generate': False,\n",
      "                        'sep_token_id': None,\n",
      "                        'suppress_tokens': None,\n",
      "                        'task_specific_params': None,\n",
      "                        'temperature': 1.0,\n",
      "                        'tf_legacy_loss': False,\n",
      "                        'tie_encoder_decoder': False,\n",
      "                        'tie_word_embeddings': True,\n",
      "                        'tokenizer_class': None,\n",
      "                        'top_k': 50,\n",
      "                        'top_p': 1.0,\n",
      "                        'torch_dtype': None,\n",
      "                        'torchscript': False,\n",
      "                        'transformers_version': '4.35.0',\n",
      "                        'type_vocab_size': 1,\n",
      "                        'typical_p': 1.0,\n",
      "                        'use_bfloat16': False,\n",
      "                        'use_cache': True,\n",
      "                        'vocab_size': 50265},\n",
      " 'classification_head': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                         'type': 'SingleClassificationHead'},\n",
      " 'embeddings': {'params': {'donor': 'textattack/roberta-base-RTE'},\n",
      "                'type': 'SingleEmbeddings'},\n",
      " 'glue_task': 'rte',\n",
      " 'layer_assignments': [{'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 0,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 1,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 2,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 3,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 4,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 5,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 6,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 7,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 8,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 9,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 10,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'},\n",
      "                       {'params': {'donors': [{'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': None,\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': 1.0},\n",
      "                                              {'donor': 'textattack/roberta-base-RTE',\n",
      "                                               'element_wise_multiplier_filename': '../data/fisher_info/textattack_roberta-base-RTE-fisher-info.h5',\n",
      "                                               'hidden_layer_number': 11,\n",
      "                                               'weight': -1.0}],\n",
      "                                   'normalize': True},\n",
      "                        'type': 'ElementWiseLinearCombination'}],\n",
      " 'tokenizer_model_id': 'textattack/roberta-base-RTE'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for config in configs:\n",
    "    print(\"==========\")\n",
    "    print(\"==========\")\n",
    "    print(\"==========\")\n",
    "    pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
