{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the weaving code on the base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: /home/brian/2023-fall-cs-194-294-merging-llms/.venv/bin/pip: bad interpreter: .venv/bin/python3.8: no such file or directory\n",
      "zsh:1: /home/brian/2023-fall-cs-194-294-merging-llms/.venv/bin/pip: bad interpreter: .venv/bin/python3.8: no such file or directory\n"
     ]
    }
   ],
   "source": [
    "# install dependencies\n",
    "\n",
    "! pip install -q joblib  # joblib for memoizing functions\n",
    "! pip install -q ipywidgets widgetsnbextension pandas-profiling # IProgress for progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add model_merging to the python path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "model_merging_base = os.path.abspath(\"../model_merging/\")\n",
    "# assert it exist\n",
    "assert os.path.exists(model_merging_base)\n",
    "if model_merging_base not in sys.path:\n",
    "    sys.path.append(model_merging_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib for caching and distributed computing\n",
    "from math import sqrt\n",
    "\n",
    "from joblib import Memory, Parallel, delayed\n",
    "\n",
    "# memory = Memory(location=\"cache\", verbose=10)\n",
    "memory = Memory(location=\"cache\", verbose=0)\n",
    "\n",
    "parallel = Parallel(n_jobs=2, return_as=\"generator\")\n",
    "output_generator = parallel(delayed(sqrt)(i**2) for i in range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 15:06:08.264426: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 15:06:08.298760: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-26 15:06:08.299324: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-26 15:06:08.837405: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Imports and cached functions\n",
    "\n",
    "import os\n",
    "\n",
    "from llm_weaver import (\n",
    "    calculate_score_from_weaving_config,\n",
    "    get_score_from_named_model,\n",
    "    test_weaver,\n",
    "    ca,\n",
    ")\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "calculate_score_from_weaving_config_cached = memory.cache(\n",
    "    calculate_score_from_weaving_config\n",
    ")\n",
    "test_weaver_cached = memory.cache(test_weaver)\n",
    "\n",
    "get_score_from_named_model_cached = memory.cache(get_score_from_named_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure you can build using `.build()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have transformers version 4.35.0!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-26 15:06:11.802542: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-11-26 15:06:11.803269: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.models.roberta.modeling_tf_roberta.TFRobertaForSequenceClassification"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from llm_weaver import get_blank_model, get_model_config\n",
    "\n",
    "if transformers.__version__ < \"4.3.1\":\n",
    "    raise ValueError(\n",
    "        \"Need transformers >= 4.3.1, or something like that. Not sure of the version.\"\n",
    "    )\n",
    "    # https://github.com/huggingface/transformers/commit/4a55e4787760fdb6c40a972a60d814ba05425da1#diff-648ec06beb5ae6380c7f611a0f513a5d392509497d245a09f06b6549358afdffR1151\n",
    "\n",
    "print(f\"You have transformers version {transformers.__version__}!\")\n",
    "\n",
    "model = get_blank_model(get_model_config(\"textattack/roberta-base-RTE\"))\n",
    "model.build()\n",
    "\n",
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Get cross-task scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ids_grouped = [\n",
    "    [\n",
    "        \"textattack/roberta-base-RTE\",\n",
    "        \"textattack/roberta-base-MNLI\",\n",
    "    ],  # <--- this one has a very low score\n",
    "    [\"howey/roberta-large-rte\", \"howey/roberta-large-mnli\"],\n",
    "    # \"howey/roberta-large-qnli\",\n",
    "    # \"howey/roberta-large-sst2\",\n",
    "    # \"howey/roberta-large-cola\",\n",
    "    # \"howey/roberta-large-mrpc\",\n",
    "    # \"howey/roberta-large-qqp\",\n",
    "    # \"howey/roberta-large-stsb\",\n",
    "    [\"JeremiahZ/roberta-base-rte\", \"JeremiahZ/roberta-base-mnli\"],\n",
    "    # \"JeremiahZ/roberta-base-qnli\",\n",
    "    # \"JeremiahZ/roberta-base-sst2\",\n",
    "    # \"JeremiahZ/roberta-base-cola\",\n",
    "    # \"JeremiahZ/roberta-base-mrpc\",\n",
    "    # \"JeremiahZ/roberta-base-qqp\",\n",
    "    # \"JeremiahZ/roberta-base-stsb\",\n",
    "    # \"l-yohai/bigbird-roberta-base-mnli\",\n",
    "    # \"howey/roberta-large-squad2\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llm_weaver import dict_overwrite, get_model_config, normalize_glue_task_name\n",
    "\n",
    "from model_merging import hdf5_util, sample_layers\n",
    "\n",
    "\n",
    "def multi_task_configs_iter(model_ids_grouped, max_configs=None):\n",
    "    # Extract the task names from the model ids\n",
    "    task_to_model_ids_map_list = [\n",
    "        {normalize_glue_task_name(model_id): model_id for model_id in group}\n",
    "        for group in model_ids_grouped\n",
    "    ]\n",
    "\n",
    "    num_configs = 0\n",
    "    for task_to_model_ids_map in task_to_model_ids_map_list:\n",
    "        tasks = list(task_to_model_ids_map.keys())\n",
    "        model_ids = list(task_to_model_ids_map.values())\n",
    "\n",
    "        for task in tasks:\n",
    "            # Use the task model as the \"blank model\"\n",
    "            task_model_id = task_to_model_ids_map[task]\n",
    "            task_model_config = get_model_config(task_model_id)\n",
    "\n",
    "            for model_id in model_ids:\n",
    "                config = {\n",
    "                    \"glue_task\": task,\n",
    "                    \"tokenizer_model_id\": task_model_id,\n",
    "                    # The task (i.e. the classification head output size should match the task at hand)\n",
    "                    \"blank_model_config\": task_model_config,\n",
    "                    # Layer assignments\n",
    "                    \"layer_assignments\": [\n",
    "                        {\n",
    "                            \"type\": \"SingleLayer\",\n",
    "                            \"params\": {\n",
    "                                \"donor\": model_id,\n",
    "                                \"hidden_layer_number\": i,\n",
    "                            },\n",
    "                        }\n",
    "                        for i in range(task_model_config[\"num_hidden_layers\"])\n",
    "                    ],\n",
    "                    # The head (i.e. the classification head should match the task at hand)\n",
    "                    # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "                    \"classification_head\": {\n",
    "                        \"type\": \"SingleClassificationHead\",\n",
    "                        \"params\": {\n",
    "                            \"donor\": task_model_id,\n",
    "                        },\n",
    "                    },\n",
    "                    # The embeddings layer\n",
    "                    # THESE ARE DIFFERENT BETWEEN RTE AND MNLI\n",
    "                    \"embeddings\": {\n",
    "                        \"type\": \"SingleEmbeddings\",\n",
    "                        \"params\": {\n",
    "                            \"donor\": model_id,\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "                num_configs += 1\n",
    "                if max_configs and num_configs > max_configs:\n",
    "                    break\n",
    "                yield config\n",
    "        # yield generate_layer_config(fishers, sample_config)\n",
    "    # # Need to decide what to do about embeddings and classification head.\n",
    "\n",
    "\n",
    "# sample_layers.generate_fisher_distributions(fishers)\n",
    "\n",
    "\n",
    "len(\n",
    "    list(\n",
    "        multi_task_configs_iter(\n",
    "            model_ids_grouped=model_ids_grouped,\n",
    "            # max_configs=4,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step get original model baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:00<00:00, 1537.71it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1681.76it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 1742.54it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 74.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_id</th>\n",
       "      <th>split</th>\n",
       "      <th>n_examples</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>matthews_correlation</th>\n",
       "      <th>f1</th>\n",
       "      <th>task</th>\n",
       "      <th>roberta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>JeremiahZ/roberta-base-cola</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>JeremiahZ/roberta-base-cola</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.452776</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>JeremiahZ/roberta-base-cola</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.172932</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>howey/roberta-large-cola</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>howey/roberta-large-cola</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.451394</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>howey/roberta-large-cola</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td></td>\n",
       "      <td>0.235292</td>\n",
       "      <td></td>\n",
       "      <td>cola</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.296875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339844</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.234375</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.960938</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>textattack/roberta-base-MNLI</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.265625</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>JeremiahZ/roberta-base-mnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.855469</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.324219</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.8125</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>howey/roberta-large-mnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.707031</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>mnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>JeremiahZ/roberta-base-mrpc</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.035156</td>\n",
       "      <td></td>\n",
       "      <td>0.067925</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>JeremiahZ/roberta-base-mrpc</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.339844</td>\n",
       "      <td></td>\n",
       "      <td>0.076503</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>JeremiahZ/roberta-base-mrpc</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.328125</td>\n",
       "      <td></td>\n",
       "      <td>0.085106</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>howey/roberta-large-mrpc</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.347656</td>\n",
       "      <td></td>\n",
       "      <td>0.515942</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>howey/roberta-large-mrpc</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.632812</td>\n",
       "      <td></td>\n",
       "      <td>0.635659</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>howey/roberta-large-mrpc</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.644531</td>\n",
       "      <td></td>\n",
       "      <td>0.678445</td>\n",
       "      <td>mrpc</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>JeremiahZ/roberta-base-qnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.492188</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>JeremiahZ/roberta-base-qnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.914062</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>JeremiahZ/roberta-base-qnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.835938</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>howey/roberta-large-qnli</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.476562</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>howey/roberta-large-qnli</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.960938</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>howey/roberta-large-qnli</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.886719</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>qnli</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>JeremiahZ/roberta-base-qqp</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.292969</td>\n",
       "      <td></td>\n",
       "      <td>0.453172</td>\n",
       "      <td>qqp</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>JeremiahZ/roberta-base-qqp</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.875</td>\n",
       "      <td></td>\n",
       "      <td>0.854545</td>\n",
       "      <td>qqp</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>JeremiahZ/roberta-base-qqp</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.847656</td>\n",
       "      <td></td>\n",
       "      <td>0.797927</td>\n",
       "      <td>qqp</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>howey/roberta-large-qqp</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.085938</td>\n",
       "      <td></td>\n",
       "      <td>0.158273</td>\n",
       "      <td>qqp</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>howey/roberta-large-qqp</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.769531</td>\n",
       "      <td></td>\n",
       "      <td>0.598639</td>\n",
       "      <td>qqp</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>howey/roberta-large-qqp</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.78125</td>\n",
       "      <td></td>\n",
       "      <td>0.575758</td>\n",
       "      <td>qqp</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.617188</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.796875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.796875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.660156</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>textattack/roberta-base-RTE</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.726562</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>JeremiahZ/roberta-base-rte</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.621094</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.25</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.691406</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>howey/roberta-large-rte</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.644531</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>rte</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>JeremiahZ/roberta-base-sst2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.433594</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>JeremiahZ/roberta-base-sst2</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.738281</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>JeremiahZ/roberta-base-sst2</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.867188</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>howey/roberta-large-sst2</td>\n",
       "      <td>test</td>\n",
       "      <td>256</td>\n",
       "      <td>0.6875</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>howey/roberta-large-sst2</td>\n",
       "      <td>train</td>\n",
       "      <td>256</td>\n",
       "      <td>0.75</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>howey/roberta-large-sst2</td>\n",
       "      <td>validation</td>\n",
       "      <td>256</td>\n",
       "      <td>0.828125</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>sst-2</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model_id       split  n_examples  accuracy  \\\n",
       "45   JeremiahZ/roberta-base-cola        test         256             \n",
       "13   JeremiahZ/roberta-base-cola       train         256             \n",
       "29   JeremiahZ/roberta-base-cola  validation         256             \n",
       "38      howey/roberta-large-cola        test         256             \n",
       "6       howey/roberta-large-cola       train         256             \n",
       "22      howey/roberta-large-cola  validation         256             \n",
       "33  textattack/roberta-base-MNLI        test         256  0.296875   \n",
       "42   JeremiahZ/roberta-base-mnli        test         256  0.339844   \n",
       "1   textattack/roberta-base-MNLI       train         256  0.234375   \n",
       "10   JeremiahZ/roberta-base-mnli       train         256  0.960938   \n",
       "17  textattack/roberta-base-MNLI  validation         256  0.265625   \n",
       "26   JeremiahZ/roberta-base-mnli  validation         256  0.855469   \n",
       "35      howey/roberta-large-mnli        test         256  0.324219   \n",
       "3       howey/roberta-large-mnli       train         256    0.8125   \n",
       "19      howey/roberta-large-mnli  validation         256  0.707031   \n",
       "46   JeremiahZ/roberta-base-mrpc        test         256  0.035156   \n",
       "14   JeremiahZ/roberta-base-mrpc       train         256  0.339844   \n",
       "30   JeremiahZ/roberta-base-mrpc  validation         256  0.328125   \n",
       "39      howey/roberta-large-mrpc        test         256  0.347656   \n",
       "7       howey/roberta-large-mrpc       train         256  0.632812   \n",
       "23      howey/roberta-large-mrpc  validation         256  0.644531   \n",
       "43   JeremiahZ/roberta-base-qnli        test         256  0.492188   \n",
       "11   JeremiahZ/roberta-base-qnli       train         256  0.914062   \n",
       "27   JeremiahZ/roberta-base-qnli  validation         256  0.835938   \n",
       "36      howey/roberta-large-qnli        test         256  0.476562   \n",
       "4       howey/roberta-large-qnli       train         256  0.960938   \n",
       "20      howey/roberta-large-qnli  validation         256  0.886719   \n",
       "47    JeremiahZ/roberta-base-qqp        test         256  0.292969   \n",
       "15    JeremiahZ/roberta-base-qqp       train         256     0.875   \n",
       "31    JeremiahZ/roberta-base-qqp  validation         256  0.847656   \n",
       "40       howey/roberta-large-qqp        test         256  0.085938   \n",
       "8        howey/roberta-large-qqp       train         256  0.769531   \n",
       "24       howey/roberta-large-qqp  validation         256   0.78125   \n",
       "32   textattack/roberta-base-RTE        test         256  0.617188   \n",
       "41    JeremiahZ/roberta-base-rte        test         256  0.796875   \n",
       "0    textattack/roberta-base-RTE       train         256  0.796875   \n",
       "9     JeremiahZ/roberta-base-rte       train         256  0.660156   \n",
       "16   textattack/roberta-base-RTE  validation         256  0.726562   \n",
       "25    JeremiahZ/roberta-base-rte  validation         256  0.621094   \n",
       "34       howey/roberta-large-rte        test         256      0.25   \n",
       "2        howey/roberta-large-rte       train         256  0.691406   \n",
       "18       howey/roberta-large-rte  validation         256  0.644531   \n",
       "44   JeremiahZ/roberta-base-sst2        test         256  0.433594   \n",
       "12   JeremiahZ/roberta-base-sst2       train         256  0.738281   \n",
       "28   JeremiahZ/roberta-base-sst2  validation         256  0.867188   \n",
       "37      howey/roberta-large-sst2        test         256    0.6875   \n",
       "5       howey/roberta-large-sst2       train         256      0.75   \n",
       "21      howey/roberta-large-sst2  validation         256  0.828125   \n",
       "\n",
       "   matthews_correlation        f1   task roberta  \n",
       "45                  0.0             cola    base  \n",
       "13             0.452776             cola    base  \n",
       "29             0.172932             cola    base  \n",
       "38                  0.0             cola   large  \n",
       "6              0.451394             cola   large  \n",
       "22             0.235292             cola   large  \n",
       "33                                  mnli    base  \n",
       "42                                  mnli    base  \n",
       "1                                   mnli    base  \n",
       "10                                  mnli    base  \n",
       "17                                  mnli    base  \n",
       "26                                  mnli    base  \n",
       "35                                  mnli   large  \n",
       "3                                   mnli   large  \n",
       "19                                  mnli   large  \n",
       "46                       0.067925   mrpc    base  \n",
       "14                       0.076503   mrpc    base  \n",
       "30                       0.085106   mrpc    base  \n",
       "39                       0.515942   mrpc   large  \n",
       "7                        0.635659   mrpc   large  \n",
       "23                       0.678445   mrpc   large  \n",
       "43                                  qnli    base  \n",
       "11                                  qnli    base  \n",
       "27                                  qnli    base  \n",
       "36                                  qnli   large  \n",
       "4                                   qnli   large  \n",
       "20                                  qnli   large  \n",
       "47                       0.453172    qqp    base  \n",
       "15                       0.854545    qqp    base  \n",
       "31                       0.797927    qqp    base  \n",
       "40                       0.158273    qqp   large  \n",
       "8                        0.598639    qqp   large  \n",
       "24                       0.575758    qqp   large  \n",
       "32                                   rte    base  \n",
       "41                                   rte    base  \n",
       "0                                    rte    base  \n",
       "9                                    rte    base  \n",
       "16                                   rte    base  \n",
       "25                                   rte    base  \n",
       "34                                   rte   large  \n",
       "2                                    rte   large  \n",
       "18                                   rte   large  \n",
       "44                                 sst-2    base  \n",
       "12                                 sst-2    base  \n",
       "28                                 sst-2    base  \n",
       "37                                 sst-2   large  \n",
       "5                                  sst-2   large  \n",
       "21                                 sst-2   large  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_weaver import normalize_glue_task_name\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_examples = 256\n",
    "\n",
    "records = []\n",
    "for split in tqdm(\n",
    "    [\n",
    "        # \"train\",\n",
    "        \"validation\",\n",
    "        # \"test\"\n",
    "    ]\n",
    "):\n",
    "    for config in tqdm(\n",
    "        multi_task_configs_iter(\n",
    "            model_ids_grouped=model_ids_grouped,\n",
    "        )\n",
    "    ):\n",
    "        records.append(\n",
    "            {\n",
    "                \"task\": config[\"glue_task\"],\n",
    "                \"classification_head_model\": config[\"classification_head\"][\"params\"][\n",
    "                    \"donor\"\n",
    "                ],\n",
    "                \"layers_models\": list(\n",
    "                    sorted(\n",
    "                        set(\n",
    "                            [\n",
    "                                layer[\"params\"][\"donor\"]\n",
    "                                for layer in config[\"layer_assignments\"]\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                ),\n",
    "                \"score\": calculate_score_from_weaving_config_cached(\n",
    "                    weaving_config=config,\n",
    "                    split=split,\n",
    "                    n_examples=n_examples,\n",
    "                ),\n",
    "                \"split\": split,\n",
    "                \"n_examples\": n_examples,\n",
    "            }\n",
    "        )\n",
    "import pandas as pd\n",
    "\n",
    "# Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
    "df = pd.DataFrame.from_records(records)\n",
    "df = df.join(pd.json_normalize(df[\"score\"])).drop(columns=[\"score\"])\n",
    "# df[\"task\"] = df[\"model_id\"].apply(normalize_glue_task_name)\n",
    "# df[\"roberta\"] = df[\"model_id\"].apply(lambda x: \"large\" if \"large\" in x else \"base\")\n",
    "# df = df[df[\"split\"] == \"train\"]\n",
    "# df = df[~df[\"accuracy\"].isna()]\n",
    "# df = df.sort_values([\"task\", \"roberta\", \"split\"])\n",
    "# replace nan with ''\n",
    "df = df.fillna(\"\")\n",
    "# df.to_csv(\"test-weaving-on-base-models.original-scores.csv\", index=False)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
